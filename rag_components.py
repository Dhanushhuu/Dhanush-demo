# Databricks notebook source
# MAGIC %sql
# MAGIC CREATE VOLUME if not exists vol_databricks_data

# COMMAND ----------

# MAGIC %pip install mlflow langchain databricks-langchain pydantic databricks-agents langchain-community

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %pip install numpy==1.26.4

# COMMAND ----------

import pkg_resources
numpy_version=pkg_resources.get_distribution("numpy").version
print(numpy_version)

# COMMAND ----------

# MAGIC %pip install --upgrade llama-index transformers
# MAGIC %pip install pypdf pdfplumber pymupdf pdfminer.six
# MAGIC %pip install python-pptx python-docx unstructured[pdf,docx,pptx]
# MAGIC %pip install ftfy regex beautifulsoup4 lxml nltk spacy langdetect
# MAGIC %pip install rank-bm25 sentence-transformers tiktoken rapidfuzz
# MAGIC dbutils.library.restartPython()
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC -- ============================================================
# MAGIC -- TABLE 1: Document Metadata
# MAGIC -- ============================================================
# MAGIC CREATE TABLE IF NOT EXISTS cvip_documents (
# MAGIC     document_id STRING NOT NULL COMMENT 'UUID - uniqueness enforced via MERGE',
# MAGIC     
# MAGIC     source_name STRING NOT NULL,
# MAGIC     source_type STRING NOT NULL COMMENT 'textbook|survey|research_paper',
# MAGIC     source_tier INT NOT NULL COMMENT '1=Textbook, 2=Survey, 3=Research',
# MAGIC     priority_score DOUBLE NOT NULL COMMENT 'Base retrieval weight (0.0-1.0)',
# MAGIC     
# MAGIC     title STRING,
# MAGIC     authors ARRAY<STRING>,
# MAGIC     publication_year INT CHECK (publication_year >= 1950),
# MAGIC     page_count INT,
# MAGIC     
# MAGIC     url STRING,
# MAGIC     source_file_path STRING COMMENT 'DBFS/Unity Catalog path to original PDF',
# MAGIC     document_checksum STRING COMMENT 'SHA256 of source file',
# MAGIC     license STRING,
# MAGIC     
# MAGIC     language STRING COMMENT 'Default: en',
# MAGIC     domain STRING COMMENT 'Default: cvip',
# MAGIC     
# MAGIC     ingestion_batch_id STRING,
# MAGIC     created_at TIMESTAMP COMMENT 'Set during ingestion',
# MAGIC     updated_at TIMESTAMP,
# MAGIC     
# MAGIC     CONSTRAINT pk_documents PRIMARY KEY (document_id)
# MAGIC )
# MAGIC USING DELTA
# MAGIC TBLPROPERTIES (
# MAGIC     'delta.enableChangeDataFeed' = 'true',
# MAGIC     'delta.autoOptimize.optimizeWrite' = 'true'
# MAGIC )
# MAGIC COMMENT 'Document-level metadata for Trust-Aware RAG (Tier 1-3 sources)';
# MAGIC

# COMMAND ----------

# MAGIC %sql
# MAGIC -- ============================================================
# MAGIC -- TABLE 2: Chunked Content
# MAGIC -- ============================================================
# MAGIC CREATE TABLE IF NOT EXISTS cvip_chunks (
# MAGIC     chunk_pk BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC     chunk_id STRING NOT NULL COMMENT 'Stable external ID - uniqueness via MERGE',
# MAGIC     document_id STRING NOT NULL COMMENT 'FK to cvip_documents',
# MAGIC     
# MAGIC     chunk_index INT NOT NULL CHECK (chunk_index >= 0),
# MAGIC     page_number INT,
# MAGIC     section_title STRING,
# MAGIC     
# MAGIC     content STRING NOT NULL,
# MAGIC     token_count INT CHECK (token_count > 0),
# MAGIC     word_count INT CHECK (word_count > 0),
# MAGIC     char_count INT CHECK (char_count > 0),
# MAGIC     
# MAGIC     embedding_model STRING COMMENT 'Default: databricks-bge-large-en',
# MAGIC     embedding_version STRING COMMENT 'Model version/checkpoint',
# MAGIC     embedding_created_at TIMESTAMP COMMENT 'When embedding was generated',
# MAGIC     
# MAGIC     has_equations BOOLEAN COMMENT 'Default: FALSE',
# MAGIC     has_code BOOLEAN COMMENT 'Default: FALSE',
# MAGIC     has_images BOOLEAN COMMENT 'Default: FALSE',
# MAGIC     topic_tags ARRAY<STRING>,
# MAGIC     
# MAGIC     citation_label STRING COMMENT 'Pre-formatted citation for display',
# MAGIC     
# MAGIC     content_checksum STRING COMMENT 'SHA256 for deduplication',
# MAGIC     is_active BOOLEAN COMMENT 'Default: TRUE',
# MAGIC     
# MAGIC     created_at TIMESTAMP COMMENT 'Set during ingestion',
# MAGIC     updated_at TIMESTAMP,
# MAGIC     
# MAGIC     CONSTRAINT pk_chunks PRIMARY KEY (chunk_pk),
# MAGIC     CONSTRAINT fk_document FOREIGN KEY (document_id) 
# MAGIC         REFERENCES cvip_documents(document_id)
# MAGIC )
# MAGIC USING DELTA
# MAGIC TBLPROPERTIES (
# MAGIC     'delta.enableChangeDataFeed' = 'true',
# MAGIC     'delta.autoOptimize.optimizeWrite' = 'true',
# MAGIC     'delta.autoOptimize.autoCompact' = 'true'
# MAGIC )
# MAGIC COMMENT 'Chunked content for vector retrieval with trust-aware metadata';

# COMMAND ----------

# MAGIC %sql
# MAGIC -- ============================================================
# MAGIC -- TABLE 3: Query Analytics & Evaluation
# MAGIC -- ============================================================
# MAGIC CREATE TABLE IF NOT EXISTS cvip_query_logs (
# MAGIC     query_id STRING NOT NULL COMMENT 'UUID for this query',
# MAGIC     session_id STRING NOT NULL COMMENT 'User session identifier',
# MAGIC     
# MAGIC     user_query STRING NOT NULL,
# MAGIC     query_type STRING NOT NULL COMMENT 'domain|general|memory',
# MAGIC     
# MAGIC     routed_to_retrieval BOOLEAN COMMENT 'Default: TRUE',
# MAGIC     top_k INT CHECK (top_k BETWEEN 1 AND 20) COMMENT 'Default: 5',
# MAGIC     
# MAGIC     retrieved_chunk_ids ARRAY<STRING>,
# MAGIC     retrieved_scores ARRAY<DOUBLE>,
# MAGIC     
# MAGIC     generated_answer STRING,
# MAGIC     model_name STRING,
# MAGIC     
# MAGIC     support_label STRING COMMENT 'fully_supported|partially_supported|not_supported',
# MAGIC     confidence_score DOUBLE CHECK (confidence_score BETWEEN 0 AND 1),
# MAGIC     
# MAGIC     latency_ms INT CHECK (latency_ms >= 0),
# MAGIC     
# MAGIC     user_feedback STRING COMMENT 'thumbs_up|thumbs_down|null',
# MAGIC     feedback_comment STRING,
# MAGIC     
# MAGIC     error_message STRING,
# MAGIC     
# MAGIC     created_at TIMESTAMP COMMENT 'Set during logging',
# MAGIC     
# MAGIC     CONSTRAINT pk_query_logs PRIMARY KEY (query_id)
# MAGIC )
# MAGIC USING DELTA
# MAGIC TBLPROPERTIES (
# MAGIC     'delta.enableChangeDataFeed' = 'true',
# MAGIC     'delta.autoOptimize.optimizeWrite' = 'true'
# MAGIC )
# MAGIC COMMENT 'Query logs for evaluation, hallucination tracking, and performance monitoring';

# COMMAND ----------

# MAGIC %sql
# MAGIC -- ============================================================
# MAGIC -- Source Configuration: Directory-Based Classification Registry
# MAGIC -- ============================================================
# MAGIC CREATE TABLE IF NOT EXISTS cvip_source_config (
# MAGIC     tier_directory STRING PRIMARY KEY COMMENT 'Directory name in volume',
# MAGIC     source_type STRING NOT NULL COMMENT 'textbook|survey|research_paper',
# MAGIC     source_tier INT NOT NULL CHECK (source_tier BETWEEN 1 AND 3),
# MAGIC     priority_score DOUBLE NOT NULL CHECK (priority_score BETWEEN 0 AND 1),
# MAGIC     citation_icon STRING COMMENT 'Display icon for citations',
# MAGIC     description STRING COMMENT 'Tier description',
# MAGIC     created_at TIMESTAMP COMMENT 'When configuration was set'
# MAGIC )
# MAGIC USING DELTA
# MAGIC COMMENT 'Directory-based classification configuration for manual source organization';
# MAGIC
# MAGIC -- Insert the three-tier configuration
# MAGIC INSERT INTO cvip_source_config VALUES
# MAGIC     ('tier1_textbooks', 'textbook', 1, 1.0, 'üìò', 'Authoritative textbooks - foundational knowledge', current_timestamp()),
# MAGIC     ('tier2_surveys', 'survey', 2, 0.7, 'üìÑ', 'Survey papers - conceptual bridges and comparisons', current_timestamp()),
# MAGIC     ('tier3_research_papers', 'research_paper', 3, 0.5, 'üß™', 'Research papers - cutting-edge techniques', current_timestamp());
# MAGIC
# MAGIC -- Verify configuration
# MAGIC SELECT * FROM cvip_source_config ORDER BY source_tier;

# COMMAND ----------

# ============================================================
# Volume Scanner: Auto-detect classification from directories
# ============================================================

from pyspark.sql import Row
from typing import List, Dict
import os

def load_source_config() -> Dict[str, Dict]:
    """
    Load classification config from cvip_source_config table.
    Returns directory-to-metadata mapping.
    """
    config_df = spark.sql("SELECT * FROM cvip_source_config")
    config_dict = {}
    
    for row in config_df.collect():
        config_dict[row.tier_directory] = {
            'source_type': row.source_type,
            'source_tier': row.source_tier,
            'priority_score': row.priority_score,
            'citation_icon': row.citation_icon,
            'description': row.description
        }
    
    return config_dict


def scan_volume_sources(base_path: str = "/Volumes/workspace/default/vol_databricks_data") -> List[Dict]:
    """
    Scan volume for PDFs and auto-detect classification based on directory.
    
    Returns:
        List of dictionaries with file info and classification metadata
    """
    # Load configuration
    config = load_source_config()
    
    all_sources = []
    
    # Scan each tier directory
    for tier_dir, tier_config in config.items():
        tier_path = f"{base_path}/{tier_dir}"
        
        try:
            files = dbutils.fs.ls(tier_path)
            pdf_files = [f for f in files if f.name.lower().endswith('.pdf')]
            
            print(f"\n{tier_config['citation_icon']} {tier_dir}: Found {len(pdf_files)} PDFs")
            
            for file_info in pdf_files:
                source_entry = {
                    # File information
                    'file_path': file_info.path,
                    'file_name': file_info.name,
                    'file_size_bytes': file_info.size,
                    
                    # Classification (from directory location)
                    'source_type': tier_config['source_type'],
                    'source_tier': tier_config['source_tier'],
                    'priority_score': tier_config['priority_score'],
                    'citation_icon': tier_config['citation_icon'],
                    
                    # Metadata
                    'tier_directory': tier_dir,
                    'classification_method': 'manual_directory_based'
                }
                
                all_sources.append(source_entry)
                print(f"  ‚úì {file_info.name}")
                
        except Exception as e:
            print(f"  ‚ö†Ô∏è Could not access {tier_path}: {e}")
    
    return all_sources


# ============================================================
# Execute: Scan and display all sources
# ============================================================
print("=" * 60)
print("üìä SCANNING CVIP KNOWLEDGE BASE")
print("=" * 60)

sources = scan_volume_sources()

print("\n" + "=" * 60)
print("üìà CORPUS STATISTICS")
print("=" * 60)
print(f"Total Documents: {len(sources)}")
print(f"  üìò Textbooks: {sum(1 for s in sources if s['source_tier'] == 1)}")
print(f"  üìÑ Surveys: {sum(1 for s in sources if s['source_tier'] == 2)}")
print(f"  üß™ Research Papers: {sum(1 for s in sources if s['source_tier'] == 3)}")

# Verify expected counts
expected = {'textbooks': 2, 'surveys': 3, 'research_papers': 5}
actual = {
    'textbooks': sum(1 for s in sources if s['source_tier'] == 1),
    'surveys': sum(1 for s in sources if s['source_tier'] == 2),
    'research_papers': sum(1 for s in sources if s['source_tier'] == 3)
}

print("\n" + "=" * 60)
print("‚úÖ VERIFICATION")
print("=" * 60)
if actual == expected:
    print("‚úÖ All files detected correctly!")
    print(f"   Expected: 2 textbooks, 3 surveys, 5 papers")
    print(f"   Found: {actual['textbooks']} textbooks, {actual['surveys']} surveys, {actual['research_papers']} papers")
else:
    print("‚ö†Ô∏è File count mismatch:")
    print(f"   Expected: 2 textbooks, 3 surveys, 5 papers")
    print(f"   Found: {actual['textbooks']} textbooks, {actual['surveys']} surveys, {actual['research_papers']} papers")
    print("\n   Please verify files are in correct directories!")

# COMMAND ----------

# ============================================================
# Create source inventory from scan results
# ============================================================

from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType
from datetime import datetime

# Define schema for source inventory
inventory_schema = StructType([
    StructField("file_path", StringType(), False),
    StructField("file_name", StringType(), False),
    StructField("file_size_bytes", LongType(), True),
    StructField("source_type", StringType(), False),
    StructField("source_tier", IntegerType(), False),
    StructField("priority_score", DoubleType(), False),
    StructField("citation_icon", StringType(), True),
    StructField("tier_directory", StringType(), False),
    StructField("classification_method", StringType(), False),
    StructField("scan_timestamp", StringType(), False)
])

# Add timestamp to source entries
for source in sources:
    source['scan_timestamp'] = datetime.now().isoformat()

# Create DataFrame
inventory_df = spark.createDataFrame([Row(**s) for s in sources], schema=inventory_schema)

# Save to Delta table
inventory_df.write \
    .format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable("cvip_source_inventory")

print("‚úÖ Source inventory saved to cvip_source_inventory table")

# Display inventory
print("\n" + "=" * 60)
print("üìã SOURCE INVENTORY")
print("=" * 60)
display(spark.sql("""
    SELECT 
        tier_directory,
        file_name,
        source_type,
        source_tier,
        priority_score,
        citation_icon,
        ROUND(file_size_bytes / 1024.0 / 1024.0, 2) as size_mb
    FROM cvip_source_inventory
    ORDER BY source_tier, file_name
"""))

# COMMAND ----------

# MAGIC %sql
# MAGIC -- ============================================================
# MAGIC -- Verification: Cross-reference config with inventory
# MAGIC -- ============================================================
# MAGIC
# MAGIC -- Check that all tier directories are represented
# MAGIC SELECT 
# MAGIC     c.tier_directory,
# MAGIC     c.source_type,
# MAGIC     c.source_tier,
# MAGIC     COUNT(i.file_name) as file_count,
# MAGIC     ROUND(SUM(i.file_size_bytes) / 1024.0 / 1024.0, 2) as total_size_mb
# MAGIC FROM cvip_source_config c
# MAGIC LEFT JOIN cvip_source_inventory i 
# MAGIC     ON c.tier_directory = i.tier_directory
# MAGIC GROUP BY c.tier_directory, c.source_type, c.source_tier
# MAGIC ORDER BY c.source_tier;
# MAGIC

# COMMAND ----------

# ============================================================
# Helper Functions: Classification Lookup
# ============================================================

def get_classification_from_path(file_path: str) -> Dict:
    """
    Get classification metadata from file path.
    Uses directory-based manual classification.
    
    Args:
        file_path: Full path to PDF file
        
    Returns:
        Dict with source_type, source_tier, priority_score, etc.
    """
    # Query source inventory
    result = spark.sql(f"""
        SELECT 
            source_type,
            source_tier,
            priority_score,
            citation_icon,
            tier_directory
        FROM cvip_source_inventory
        WHERE file_path = '{file_path}'
    """).first()
    
    if result:
        return {
            'source_type': result.source_type,
            'source_tier': result.source_tier,
            'priority_score': result.priority_score,
            'citation_icon': result.citation_icon,
            'tier_directory': result.tier_directory,
            'classification_method': 'manual_directory_based'
        }
    else:
        raise ValueError(f"File not found in inventory: {file_path}")


def get_all_sources_by_tier(tier: int = None) -> List[Dict]:
    """
    Get all sources, optionally filtered by tier.
    Useful for processing one tier at a time.
    
    Args:
        tier: Optional tier number (1, 2, or 3)
        
    Returns:
        List of source dictionaries
    """
    if tier:
        query = f"""
            SELECT * FROM cvip_source_inventory 
            WHERE source_tier = {tier}
            ORDER BY file_name
        """
    else:
        query = """
            SELECT * FROM cvip_source_inventory 
            ORDER BY source_tier, file_name
        """
    
    results = spark.sql(query).collect()
    return [row.asDict() for row in results]


# ============================================================
# Test the helper functions
# ============================================================
print("üß™ Testing helper functions...")

# Test 1: Get all textbooks
textbooks = get_all_sources_by_tier(tier=1)
print(f"\nüìò Textbooks found: {len(textbooks)}")
for tb in textbooks:
    print(f"   - {tb['file_name']}")

# Test 2: Get all surveys
surveys = get_all_sources_by_tier(tier=2)
print(f"\nüìÑ Surveys found: {len(surveys)}")
for sv in surveys:
    print(f"   - {sv['file_name']}")

# Test 3: Get all research papers
papers = get_all_sources_by_tier(tier=3)
print(f"\nüß™ Research Papers found: {len(papers)}")
for rp in papers:
    print(f"   - {rp['file_name']}")

print("\n‚úÖ Helper functions working correctly!")

# COMMAND ----------

# ============================================================
# Generate Classification Documentation
# ============================================================

def generate_classification_report():
    """Generate markdown documentation of manual classification"""
    
    report = """
# CVIP Knowledge Base - Manual Classification Report

## Classification Methodology
**Method:** Manual directory-based classification  
**Principle:** Files organized by human expert into tier directories  
**Rationale:** Small, curated corpus (10 documents) - manual classification ensures 100% accuracy

---

## Corpus Composition

"""
    
    # Get statistics
    for tier in [1, 2, 3]:
        sources = get_all_sources_by_tier(tier)
        tier_names = {1: 'Textbooks', 2: 'Surveys', 3: 'Research Papers'}
        tier_icons = {1: 'üìò', 2: 'üìÑ', 3: 'üß™'}
        
        report += f"### {tier_icons[tier]} Tier {tier}: {tier_names[tier]}\n"
        report += f"**Count:** {len(sources)} documents  \n"
        report += f"**Priority Score:** {sources[0]['priority_score'] if sources else 'N/A'}  \n\n"
        
        report += "**Documents:**\n"
        for i, src in enumerate(sources, 1):
            report += f"{i}. `{src['file_name']}`\n"
        
        report += "\n---\n\n"
    
    report += """
## Directory Structure
```
/Volumes/main/default/cvip_sources/
‚îú‚îÄ‚îÄ tier1_textbooks/        (2 PDFs - Priority: 1.0)
‚îú‚îÄ‚îÄ tier2_surveys/          (3 PDFs - Priority: 0.7)
‚îî‚îÄ‚îÄ tier3_research_papers/  (5 PDFs - Priority: 0.5)
```

## Classification Verification

‚úÖ All 10 documents successfully classified  
‚úÖ No orphaned or unclassified files  
‚úÖ Directory structure matches system configuration  

**Total Documents:** 10  
**Classification Accuracy:** 100% (manual expert classification)  
**Classification Date:** """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """

---

*This classification is stored in the `cvip_source_inventory` table and referenced during ingestion.*
"""
    
    return report

# Generate and display report
classification_report = generate_classification_report()
print(classification_report)

# COMMAND ----------

# MAGIC %pip install --quiet pymupdf langchain langchain-community tiktoken
# MAGIC %pip install -U langchain-text-splitters
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# ============================================================
# CVIP Trust-Aware PDF Processing Pipeline
# ============================================================

import hashlib
import uuid
from datetime import datetime
from typing import Dict, List, Tuple
import fitz  # PyMuPDF
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tiktoken

# ============================================================
# TIER-SPECIFIC CHUNKING CONFIGURATIONS
# ============================================================

CHUNKING_CONFIGS = {
    1: {  # Textbooks - need careful, larger chunks
        'chunk_size': 800,
        'chunk_overlap': 150,
        'separators': ["\n\n\n", "\n\n", "\n", ". ", " ", ""],
        'description': 'Large chunks with high overlap for continuity'
    },
    2: {  # Surveys - medium chunks
        'chunk_size': 700,
        'chunk_overlap': 120,
        'separators': ["\n\n", "\n", ". ", " ", ""],
        'description': 'Medium chunks for survey content'
    },
    3: {  # Research papers - smaller, focused chunks
        'chunk_size': 600,
        'chunk_overlap': 100,
        'separators': ["\n\n", "\n", ". ", " ", ""],
        'description': 'Smaller chunks for dense research content'
    }
}


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def calculate_checksum(content: str) -> str:
    """Calculate SHA256 checksum for content"""
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def calculate_file_checksum(file_path: str) -> str:
    """Calculate SHA256 checksum for entire file"""
    sha256_hash = hashlib.sha256()
    
    # Read file in chunks to handle large files
    with open(file_path.replace('dbfs:', '/dbfs'), 'rb') as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    
    return sha256_hash.hexdigest()


def count_tokens(text: str, model: str = "cl100k_base") -> int:
    """Count tokens using tiktoken (OpenAI tokenizer)"""
    try:
        encoding = tiktoken.get_encoding(model)
        return len(encoding.encode(text))
    except:
        # Fallback: rough estimate (1 token ‚âà 4 characters)
        return len(text) // 4


def extract_pdf_metadata(pdf_path: str) -> Dict:
    """Extract metadata from PDF using PyMuPDF"""
    actual_path = pdf_path.replace('dbfs:', '/dbfs')
    
    try:
        doc = fitz.open(actual_path)
        metadata = doc.metadata
        
        return {
            'title': metadata.get('title', ''),
            'author': metadata.get('author', ''),
            'subject': metadata.get('subject', ''),
            'keywords': metadata.get('keywords', ''),
            'creator': metadata.get('creator', ''),
            'producer': metadata.get('producer', ''),
            'page_count': len(doc)
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not extract metadata from {pdf_path}: {e}")
        return {'page_count': 0}


def has_equations(text: str) -> bool:
    """Detect if text contains mathematical equations"""
    equation_indicators = [
        '‚à´', '‚àë', '‚àè', '‚àö', '‚àÇ', '‚àá', '‚â§', '‚â•', '‚âà', '‚â†',
        r'\int', r'\sum', r'\frac', r'\sqrt', r'\partial',
        '$$', r'\[', r'\]', r'\begin{equation}'
    ]
    return any(indicator in text for indicator in equation_indicators)


def has_code(text: str) -> bool:
    """Detect if text contains code or pseudocode"""
    code_indicators = [
        'def ', 'function ', 'class ', 'import ', 'for (', 'while (',
        'Algorithm ', 'procedure ', 'return ', '```', 'for i in'
    ]
    return any(indicator in text for indicator in code_indicators)


# ============================================================
# PDF TEXT EXTRACTION
# ============================================================

def extract_text_from_pdf(pdf_path: str) -> List[Tuple[int, str]]:
    """
    Extract text from PDF with page tracking.
    
    Returns:
        List of (page_number, text) tuples
    """
    actual_path = pdf_path.replace('dbfs:', '/dbfs')
    pages_text = []
    
    try:
        doc = fitz.open(actual_path)
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text()
            
            # Clean text
            text = text.strip()
            
            # Only include pages with substantial text
            if len(text) > 50:  # Skip nearly empty pages
                pages_text.append((page_num + 1, text))  # 1-indexed page numbers
        
        doc.close()
        print(f"‚úÖ Extracted text from {len(pages_text)} pages")
        
    except Exception as e:
        print(f"‚ùå Error extracting text from {pdf_path}: {e}")
        raise
    
    return pages_text


# ============================================================
# TIER-AWARE CHUNKING
# ============================================================

def chunk_document(pages_text: List[Tuple[int, str]], source_tier: int) -> List[Dict]:
    """
    Chunk document text using tier-specific strategy.
    
    Args:
        pages_text: List of (page_number, text) tuples
        source_tier: Tier number (1, 2, or 3)
        
    Returns:
        List of chunk dictionaries with metadata
    """
    config = CHUNKING_CONFIGS[source_tier]
    
    # Initialize text splitter with tier-specific config
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config['chunk_size'],
        chunk_overlap=config['chunk_overlap'],
        separators=config['separators'],
        length_function=len
    )
    
    chunks = []
    chunk_global_index = 0
    
    # Process each page
    for page_num, page_text in pages_text:
        
        # Split page text into chunks
        page_chunks = text_splitter.split_text(page_text)
        
        for chunk_text in page_chunks:
            if len(chunk_text.strip()) < 50:  # Skip tiny chunks
                continue
            
            # Calculate metadata
            token_count = count_tokens(chunk_text)
            word_count = len(chunk_text.split())
            char_count = len(chunk_text)
            
            chunk_data = {
                'chunk_index': chunk_global_index,
                'page_number': page_num,
                'content': chunk_text.strip(),
                'token_count': token_count,
                'word_count': word_count,
                'char_count': char_count,
                'has_equations': has_equations(chunk_text),
                'has_code': has_code(chunk_text),
                'content_checksum': calculate_checksum(chunk_text)
            }
            
            chunks.append(chunk_data)
            chunk_global_index += 1
    
    print(f"‚úÖ Created {len(chunks)} chunks using Tier {source_tier} strategy")
    return chunks


print("‚úÖ PDF Processing Module Loaded")
print("   - Tier-specific chunking strategies configured")
print("   - Metadata extraction ready")
print("   - Text extraction ready")

# COMMAND ----------

# ============================================================
# DOCUMENT REGISTRATION (Phase 1)
# ============================================================

def register_document(source_info: Dict, pdf_metadata: Dict) -> str:
    """
    Register document in cvip_documents table.
    
    Args:
        source_info: Classification info from cvip_source_inventory
        pdf_metadata: Extracted PDF metadata
        
    Returns:
        document_id
    """
    from pyspark.sql import Row
    
    # Generate document ID
    document_id = str(uuid.uuid4())
    
    # Extract file name without extension for source_name if needed
    file_name = source_info['file_name']
    source_name = file_name.replace('.pdf', '').replace('_', ' ').title()
    
    # Parse authors if present in metadata
    author_str = pdf_metadata.get('author', '')
    authors = [a.strip() for a in author_str.split(',') if a.strip()] if author_str else []
    
    # Calculate document checksum
    doc_checksum = calculate_file_checksum(source_info['file_path'])
    
    # Create document entry
    doc_entry = {
        'document_id': document_id,
        'source_name': source_name,
        'source_type': source_info['source_type'],
        'source_tier': source_info['source_tier'],
        'priority_score': source_info['priority_score'],
        'title': pdf_metadata.get('title') or source_name,
        'authors': authors if authors else None,
        'publication_year': None,  # Can be manually added later
        'page_count': pdf_metadata.get('page_count', 0),
        'url': None,
        'source_file_path': source_info['file_path'],
        'document_checksum': doc_checksum,
        'license': None,
        'language': 'en',
        'domain': 'cvip',
        'ingestion_batch_id': f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'created_at': datetime.now(),
        'updated_at': None
    }
    
    # Create DataFrame and insert
    df = spark.createDataFrame([Row(**doc_entry)])
    
    df.write.format("delta") \
        .mode("append") \
        .saveAsTable("cvip_documents")
    
    print(f"‚úÖ Registered document: {source_name} (ID: {document_id})")
    
    return document_id


print("‚úÖ Document Registration Function Ready")

# COMMAND ----------

# ============================================================
# CHUNK INGESTION (Phase 2)
# ============================================================

def ingest_chunks(document_id: str, source_info: Dict, chunks: List[Dict]):
    """
    Ingest chunks into cvip_chunks table with full metadata.
    
    Args:
        document_id: UUID of parent document
        source_info: Classification info
        chunks: List of chunk dictionaries
    """
    from pyspark.sql import Row
    
    chunk_rows = []
    citation_icon = source_info['citation_icon']
    source_name = source_info['file_name'].replace('.pdf', '')
    
    for chunk in chunks:
        # Generate chunk ID
        chunk_id = f"{document_id}_chunk_{chunk['chunk_index']:04d}"
        
        # Create citation label
        citation_label = f"{citation_icon} {source_name}, p.{chunk['page_number']}"
        
        # Create chunk entry
        chunk_entry = {
            'chunk_id': chunk_id,
            'document_id': document_id,
            'chunk_index': chunk['chunk_index'],
            'page_number': chunk['page_number'],
            'section_title': None,  # Could be enhanced with section detection
            'content': chunk['content'],
            'token_count': chunk['token_count'],
            'word_count': chunk['word_count'],
            'char_count': chunk['char_count'],
            'embedding_model': 'databricks-bge-large-en',
            'embedding_version': 'v1.5',
            'embedding_created_at': None,  # Set when embeddings are created
            'has_equations': chunk['has_equations'],
            'has_code': chunk['has_code'],
            'has_images': False,  # Could be enhanced with image detection
            'topic_tags': None,  # Could add keyword extraction
            'citation_label': citation_label,
            'content_checksum': chunk['content_checksum'],
            'is_active': True,
            'created_at': datetime.now(),
            'updated_at': None
        }
        
        chunk_rows.append(Row(**chunk_entry))
    
    # Create DataFrame and insert in batch
    df = spark.createDataFrame(chunk_rows)
    
    df.write.format("delta") \
        .mode("append") \
        .saveAsTable("cvip_chunks")
    
    print(f"‚úÖ Ingested {len(chunks)} chunks for document {document_id}")


print("‚úÖ Chunk Ingestion Function Ready")

# COMMAND ----------

# ============================================================
# MISSING HELPER FUNCTION - Add this before pipeline execution
# ============================================================

from typing import List, Dict

def get_all_sources_by_tier(tier: int = None) -> List[Dict]:
    """
    Get all sources from inventory, optionally filtered by tier.
    
    Args:
        tier: Optional tier number (1, 2, or 3)
        
    Returns:
        List of source dictionaries with file paths and metadata
    """
    if tier:
        query = f"""
            SELECT 
                file_path,
                file_name,
                file_size_bytes,
                source_type,
                source_tier,
                priority_score,
                citation_icon,
                tier_directory
            FROM cvip_source_inventory 
            WHERE source_tier = {tier}
            ORDER BY file_name
        """
    else:
        query = """
            SELECT 
                file_path,
                file_name,
                file_size_bytes,
                source_type,
                source_tier,
                priority_score,
                citation_icon,
                tier_directory
            FROM cvip_source_inventory 
            ORDER BY source_tier, file_name
        """
    
    results = spark.sql(query).collect()
    return [row.asDict() for row in results]


print("‚úÖ Helper function loaded")

# Test it
print("\nüß™ Testing helper function...")
tier1_sources = get_all_sources_by_tier(tier=1)
print(f"Found {len(tier1_sources)} Tier 1 documents:")
for src in tier1_sources:
    print(f"  üìò {src['file_name']}")

# COMMAND ----------

# ============================================================
# COMPLETE CVIP INGESTION PIPELINE - FINAL VERSION
# ============================================================

import hashlib
import uuid
from datetime import datetime
from typing import Dict, List, Tuple
import fitz  # PyMuPDF
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tiktoken
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, TimestampType, BooleanType, ArrayType
)

print("üîÑ Loading FINAL Ingestion Pipeline...")

# ============================================================
# SCHEMAS
# ============================================================

DOCUMENTS_SCHEMA = StructType([
    StructField("document_id", StringType(), False),
    StructField("source_name", StringType(), False),
    StructField("source_type", StringType(), False),
    StructField("source_tier", IntegerType(), False),
    StructField("priority_score", DoubleType(), False),
    StructField("title", StringType(), True),
    StructField("authors", ArrayType(StringType()), True),
    StructField("publication_year", IntegerType(), True),
    StructField("page_count", IntegerType(), True),
    StructField("url", StringType(), True),
    StructField("source_file_path", StringType(), True),
    StructField("document_checksum", StringType(), True),
    StructField("license", StringType(), True),
    StructField("language", StringType(), True),
    StructField("domain", StringType(), True),
    StructField("ingestion_batch_id", StringType(), True),
    StructField("created_at", TimestampType(), True),
    StructField("updated_at", TimestampType(), True)
])

CHUNKS_SCHEMA = StructType([
    StructField("chunk_id", StringType(), False),
    StructField("document_id", StringType(), False),
    StructField("chunk_index", IntegerType(), False),
    StructField("page_number", IntegerType(), True),
    StructField("section_title", StringType(), True),
    StructField("content", StringType(), False),
    StructField("token_count", IntegerType(), True),
    StructField("word_count", IntegerType(), True),
    StructField("char_count", IntegerType(), True),
    StructField("embedding_model", StringType(), True),
    StructField("embedding_version", StringType(), True),
    StructField("embedding_created_at", TimestampType(), True),
    StructField("has_equations", BooleanType(), True),
    StructField("has_code", BooleanType(), True),
    StructField("has_images", BooleanType(), True),
    StructField("topic_tags", ArrayType(StringType()), True),
    StructField("citation_label", StringType(), True),
    StructField("content_checksum", StringType(), True),
    StructField("is_active", BooleanType(), True),
    StructField("created_at", TimestampType(), True),
    StructField("updated_at", TimestampType(), True)
])

# ============================================================
# PATH HANDLING
# ============================================================

def get_local_path(file_path: str) -> str:
    """Convert to local path"""
    path = file_path.replace('dbfs:', '')
    return path if path.startswith('/Volumes/') else '/dbfs' + path

# ============================================================
# HELPERS
# ============================================================

def get_all_sources_by_tier(tier: int = None) -> List[Dict]:
    """Get sources"""
    query = f"SELECT * FROM cvip_source_inventory WHERE source_tier = {tier}" if tier else "SELECT * FROM cvip_source_inventory"
    return [row.asDict() for row in spark.sql(query + " ORDER BY file_name").collect()]

def calculate_checksum(content: str) -> str:
    return hashlib.sha256(content.encode('utf-8')).hexdigest()

def calculate_file_checksum(file_path: str) -> str:
    sha256_hash = hashlib.sha256()
    with open(get_local_path(file_path), 'rb') as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def count_tokens(text: str) -> int:
    try:
        return len(tiktoken.get_encoding("cl100k_base").encode(text))
    except:
        return len(text) // 4

def has_equations(text: str) -> bool:
    return any(ind in text for ind in ['‚à´', '‚àë', '‚àö', '‚àÇ', r'\int', r'\sum', '$$'])

def has_code(text: str) -> bool:
    return any(ind in text for ind in ['def ', 'function ', 'Algorithm ', '```'])

# ============================================================
# CHUNKING
# ============================================================

CHUNKING_CONFIGS = {
    1: {'chunk_size': 800, 'chunk_overlap': 150, 'separators': ["\n\n\n", "\n\n", "\n", ". ", " ", ""]},
    2: {'chunk_size': 700, 'chunk_overlap': 120, 'separators': ["\n\n", "\n", ". ", " ", ""]},
    3: {'chunk_size': 600, 'chunk_overlap': 100, 'separators': ["\n\n", "\n", ". ", " ", ""]}
}

# ============================================================
# PDF PROCESSING
# ============================================================

def extract_pdf_metadata(pdf_path: str) -> Dict:
    try:
        doc = fitz.open(get_local_path(pdf_path))
        metadata = doc.metadata
        return {'title': metadata.get('title', ''), 'author': metadata.get('author', ''), 'page_count': len(doc)}
    except Exception as e:
        print(f"‚ö†Ô∏è Metadata error: {e}")
        return {'page_count': 0}

def extract_text_from_pdf(pdf_path: str) -> List[Tuple[int, str]]:
    doc = fitz.open(get_local_path(pdf_path))
    pages = [(i+1, doc[i].get_text().strip()) for i in range(len(doc)) if len(doc[i].get_text().strip()) > 50]
    doc.close()
    print(f"‚úÖ Extracted {len(pages)} pages")
    return pages

def chunk_document(pages_text: List[Tuple[int, str]], source_tier: int) -> List[Dict]:
    config = CHUNKING_CONFIGS[source_tier]
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=config['chunk_size'],
        chunk_overlap=config['chunk_overlap'],
        separators=config['separators']
    )
    
    chunks = []
    idx = 0
    for page_num, page_text in pages_text:
        for chunk_text in splitter.split_text(page_text):
            if len(chunk_text.strip()) >= 50:
                chunks.append({
                    'chunk_index': idx,
                    'page_number': page_num,
                    'content': chunk_text.strip(),
                    'token_count': count_tokens(chunk_text),
                    'word_count': len(chunk_text.split()),
                    'char_count': len(chunk_text),
                    'has_equations': has_equations(chunk_text),
                    'has_code': has_code(chunk_text),
                    'content_checksum': calculate_checksum(chunk_text)
                })
                idx += 1
    
    print(f"‚úÖ {len(chunks)} chunks (Tier {source_tier})")
    return chunks

# ============================================================
# DATABASE INGESTION
# ============================================================

def register_document(source_info: Dict, pdf_metadata: Dict) -> str:
    document_id = str(uuid.uuid4())
    source_name = source_info['file_name'].replace('.pdf', '')
    authors = [a.strip() for a in pdf_metadata.get('author', '').split(',') if a.strip()] or None
    
    doc_entry = {
        'document_id': document_id,
        'source_name': source_name,
        'source_type': source_info['source_type'],
        'source_tier': int(source_info['source_tier']),
        'priority_score': float(source_info['priority_score']),
        'title': pdf_metadata.get('title') or source_name,
        'authors': authors,
        'publication_year': None,
        'page_count': int(pdf_metadata.get('page_count', 0)),
        'url': None,
        'source_file_path': source_info['file_path'],
        'document_checksum': calculate_file_checksum(source_info['file_path']),
        'license': None,
        'language': 'en',
        'domain': 'cvip',
        'ingestion_batch_id': f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'created_at': datetime.now(),
        'updated_at': None
    }
    
    spark.createDataFrame([doc_entry], schema=DOCUMENTS_SCHEMA).write.format("delta").mode("append").saveAsTable("cvip_documents")
    print(f"‚úÖ Registered: {source_name}")
    return document_id

def ingest_chunks(document_id: str, source_info: Dict, chunks: List[Dict]):
    chunk_data = [{
        'chunk_id': f"{document_id}_chunk_{c['chunk_index']:04d}",
        'document_id': document_id,
        'chunk_index': int(c['chunk_index']),
        'page_number': int(c['page_number']),
        'section_title': None,
        'content': c['content'],
        'token_count': int(c['token_count']),
        'word_count': int(c['word_count']),
        'char_count': int(c['char_count']),
        'embedding_model': 'databricks-bge-large-en',
        'embedding_version': 'v1.5',
        'embedding_created_at': None,
        'has_equations': bool(c['has_equations']),
        'has_code': bool(c['has_code']),
        'has_images': False,
        'topic_tags': None,
        'citation_label': f"{source_info['citation_icon']} {source_info['file_name'].replace('.pdf', '')}, p.{c['page_number']}",
        'content_checksum': c['content_checksum'],
        'is_active': True,
        'created_at': datetime.now(),
        'updated_at': None
    } for c in chunks]
    
    spark.createDataFrame(chunk_data, schema=CHUNKS_SCHEMA).write.format("delta").mode("append").saveAsTable("cvip_chunks")
    print(f"‚úÖ {len(chunks)} chunks ingested")

# ============================================================
# MAIN PIPELINE
# ============================================================

def process_single_document(source_info: Dict) -> Dict:
    print(f"\n{'='*60}\nüîÑ {source_info['file_name']}\n   Tier: {source_info['source_tier']} | Priority: {source_info['priority_score']}\n{'='*60}")
    
    try:
        print("\nüìã Phase 1: Registration")
        pdf_metadata = extract_pdf_metadata(source_info['file_path'])
        document_id = register_document(source_info, pdf_metadata)
        
        print("\nüìù Phase 2: Text Extraction")
        pages_text = extract_text_from_pdf(source_info['file_path'])
        
        print("\n‚úÇÔ∏è Phase 3: Chunking")
        chunks = chunk_document(pages_text, source_info['source_tier'])
        
        print("\nüíæ Phase 4: Ingestion")
        ingest_chunks(document_id, source_info, chunks)
        
        stats = {'file_name': source_info['file_name'], 'status': 'success', 'pages': len(pages_text), 'chunks': len(chunks), 'tokens': sum(c['token_count'] for c in chunks)}
        print(f"\n‚úÖ SUCCESS - Pages: {stats['pages']}, Chunks: {stats['chunks']}, Tokens: {stats['tokens']:,}")
        return stats
    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'file_name': source_info['file_name'], 'status': 'failed', 'error': str(e)}

def process_all_documents(tier_filter: int = None):
    sources = get_all_sources_by_tier(tier_filter)
    print(f"üéØ Processing {len(sources)} documents (Tier {tier_filter or 'ALL'})")
    
    results = [process_single_document(s) for s in sources]
    
    successful = [r for r in results if r['status'] == 'success']
    failed = [r for r in results if r['status'] == 'failed']
    
    print(f"\n{'='*60}\nüìä SUMMARY\n{'='*60}")
    print(f"‚úÖ Success: {len(successful)} | ‚ùå Failed: {len(failed)}")
    
    if successful:
        print(f"üì¶ Chunks: {sum(r['chunks'] for r in successful):,}")
        print(f"üî¢ Tokens: {sum(r['tokens'] for r in successful):,}")
    
    if failed:
        print("\n‚ö†Ô∏è Failed:")
        for r in failed:
            print(f"   - {r['file_name']}")
    
    return results

print("‚úÖ FINAL Pipeline Ready!")

# COMMAND ----------

# Test path conversion
test_path = "/Volumes/workspace/default/vol_databricks_data/tier1_textbooks/test.pdf"
converted = get_local_path(test_path)
print(f"Original: {test_path}")
print(f"Converted: {converted}")

# Verify files are accessible
import os
tier1_path = "/Volumes/workspace/default/vol_databricks_data/tier1_textbooks"
if os.path.exists(tier1_path):
    print(f"\n‚úÖ Directory exists: {tier1_path}")
    files = os.listdir(tier1_path)
    print(f"üìÅ Found {len(files)} files:")
    for f in files:
        print(f"   - {f}")
else:
    print(f"\n‚ùå Directory not found: {tier1_path}")

# COMMAND ----------

print("üöÄ Starting Tier 1 ingestion (FINAL FIX)...")
results_tier1 = process_all_documents(tier_filter=1)

# COMMAND ----------

# Process Tier 2 (Surveys)
print("Starting Tier 2 (Surveys) ingestion...")
results_tier2 = process_all_documents(tier_filter=2)

# COMMAND ----------

# Process Tier 3 (Research Papers)
print("Starting Tier 3 (Research Papers) ingestion...")
results_tier3 = process_all_documents(tier_filter=3)

# COMMAND ----------

# ============================================================
# DISCOVER ACTUAL CATALOG/SCHEMA STRUCTURE
# ============================================================

print("üîç Discovering your Databricks catalog structure...")
print("=" * 60)

# Method 1: Check current catalog and schema
print("\nüìä Method 1: Current Session Info")
current_info = spark.sql("SELECT current_catalog() as catalog, current_schema() as schema").collect()[0]
print(f"   Current Catalog: {current_info.catalog}")
print(f"   Current Schema: {current_info.schema}")

# Method 2: List all catalogs
print("\nüìö Method 2: Available Catalogs")
try:
    catalogs = spark.sql("SHOW CATALOGS").collect()
    print(f"   Found {len(catalogs)} catalogs:")
    for cat in catalogs:
        print(f"      - {cat.catalog}")
except Exception as e:
    print(f"   Could not list catalogs: {e}")

# Method 3: Check where your tables actually are
print("\nüìã Method 3: Locating Your Tables")

# Try different catalog possibilities
possible_locations = [
    "cvip_documents",
    "default.cvip_documents",
    "main.default.cvip_documents",
    "workspace.default.cvip_documents",
    "hive_metastore.default.cvip_documents"
]

table_location = None

for location in possible_locations:
    try:
        result = spark.sql(f"SELECT COUNT(*) as count FROM {location}").collect()[0]
        count = result.count
        print(f"   ‚úÖ FOUND: {location} ({count} documents)")
        table_location = location
        break
    except Exception as e:
        print(f"   ‚ùå Not here: {location}")

if table_location:
    print(f"\nüéØ Your tables are located at: {table_location}")
    
    # Extract catalog and schema
    parts = table_location.split('.')
    if len(parts) == 3:
        ACTUAL_CATALOG = parts[0]
        ACTUAL_SCHEMA = parts[1]
    elif len(parts) == 2:
        ACTUAL_CATALOG = "hive_metastore"  # Default for 2-part names
        ACTUAL_SCHEMA = parts[0]
    else:
        ACTUAL_CATALOG = "hive_metastore"
        ACTUAL_SCHEMA = "default"
    
    print(f"\n‚úÖ SOLUTION:")
    print(f"   Catalog: {ACTUAL_CATALOG}")
    print(f"   Schema: {ACTUAL_SCHEMA}")
else:
    print("\n‚ö†Ô∏è Could not find your tables automatically")
    print("   Please run: DESCRIBE TABLE EXTENDED cvip_documents")

# COMMAND ----------

# ============================================================
# CVIP VECTOR SEARCH - FIXED WITH PROPER 3-PART NAMING
# ============================================================

from databricks.vector_search.client import VectorSearchClient
import time

print("üîß Setting up CVIP Vector Search (Fixed Naming)")
print("=" * 60)

# Initialize client (simple method works fine)
vsc = VectorSearchClient(disable_notice=True)

# ============================================================
# CONFIGURATION - PROPER 3-PART NAMES
# ============================================================

ENDPOINT_NAME = "cvip_endpoint"

# Key fix: Index name MUST be <catalog>.<schema>.<index_name>
CATALOG = "workspace"
SCHEMA = "default"
INDEX_BASE_NAME = "cvip_chunks_vs_index"

# Full 3-part names (required!)
INDEX_NAME = f"{CATALOG}.{SCHEMA}.{INDEX_BASE_NAME}"
SOURCE_TABLE_NAME = f"{CATALOG}.{SCHEMA}.cvip_chunks"  # Also use 3-part for source

print(f"üìã Configuration:")
print(f"   Catalog: {CATALOG}")
print(f"   Schema: {SCHEMA}")
print(f"   Endpoint: {ENDPOINT_NAME}")
print(f"   Index (full): {INDEX_NAME}")
print(f"   Source (full): {SOURCE_TABLE_NAME}")

# ============================================================
# Verify tables with full names
# ============================================================

print(f"\nüîç Verifying source table...")
try:
    # Try with full 3-part name
    count_result = spark.sql(f"SELECT COUNT(*) as count FROM {SOURCE_TABLE_NAME}").collect()[0]
    chunk_count = count_result.count
    print(f"‚úÖ Source table verified: {chunk_count} chunks")
    
except Exception as e1:
    # Fallback: try without catalog
    print(f"   ‚ö†Ô∏è 3-part name failed, trying without catalog...")
    try:
        SOURCE_TABLE_NAME = "cvip_chunks"  # Fallback to simple name
        count_result = spark.sql(f"SELECT COUNT(*) as count FROM {SOURCE_TABLE_NAME}").collect()[0]
        chunk_count = count_result.count
        print(f"‚úÖ Source table verified (simple name): {chunk_count} chunks")
    except Exception as e2:
        print(f"‚ùå Cannot access table: {e2}")
        raise

# ============================================================
# Verify endpoint is ONLINE
# ============================================================

print(f"\nüìç Checking Endpoint Status...")

try:
    endpoint_status = vsc.get_endpoint(ENDPOINT_NAME)
    state = endpoint_status.get('endpoint_status', {}).get('state', 'UNKNOWN')
    print(f"   Endpoint '{ENDPOINT_NAME}' state: {state}")
    
    if state != 'ONLINE':
        print(f"   ‚è≥ Endpoint not ONLINE yet (state: {state})")
        print("   Please wait and re-run this cell")
        raise Exception(f"Endpoint not ready: {state}")
    
    print("   ‚úÖ Endpoint is ONLINE!")
    
except Exception as e:
    print(f"   ‚ùå Endpoint error: {e}")
    raise

# ============================================================
# Create Index with FULL 3-part names
# ============================================================

print(f"\nüìö Creating Vector Search Index...")
print(f"   Index: {INDEX_NAME}")
print(f"   Source: {SOURCE_TABLE_NAME}")

try:
    index = vsc.create_delta_sync_index(
        endpoint_name=ENDPOINT_NAME,
        index_name=INDEX_NAME,  # Full 3-part name: workspace.default.cvip_chunks_vs_index
        source_table_name=SOURCE_TABLE_NAME,  # Full 3-part or simple name
        pipeline_type="TRIGGERED",
        primary_key="chunk_id",
        embedding_source_column="content",
        embedding_model_endpoint_name="databricks-bge-large-en"
    )
    
    print(f"‚úÖ Index created successfully!")
    print(f"   Full name: {INDEX_NAME}")
    print(f"   Embedding: databricks-bge-large-en (1024 dims)")
    
    # Trigger initial sync
    print(f"\nüîÑ Triggering initial sync...")
    try:
        index_obj = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)
        index_obj.sync()
        print("‚úÖ Sync triggered!")
    except Exception as e:
        print(f"‚ÑπÔ∏è Sync will happen automatically: {e}")
    
except Exception as e:
    error_msg = str(e).lower()
    
    if "already exists" in error_msg:
        print(f"‚ÑπÔ∏è Index already exists: {INDEX_NAME}")
        print("   Triggering sync on existing index...")
        
        try:
            index_obj = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)
            index_obj.sync()
            print("‚úÖ Sync triggered!")
        except Exception as sync_err:
            print(f"‚ÑπÔ∏è Sync note: {sync_err}")
    
    elif "invalid" in error_msg and "name" in error_msg:
        print(f"‚ùå NAMING ERROR")
        print(f"   Error: {e}")
        print(f"\nüìã Attempted names:")
        print(f"   Index: {INDEX_NAME}")
        print(f"   Source: {SOURCE_TABLE_NAME}")
        print("\nüí° The index name must be exactly: <catalog>.<schema>.<index_name>")
        print("   No special characters except dots and underscores")
        raise
    
    else:
        print(f"‚ùå Error: {e}")
        raise

print("\n" + "=" * 60)
print("‚úÖ SETUP COMPLETE!")
print("=" * 60)
print(f"\nüìã Your Vector Search Setup:")
print(f"   Endpoint: {ENDPOINT_NAME}")
print(f"   Index: {INDEX_NAME}")
print(f"   Source: {SOURCE_TABLE_NAME}")
print(f"   Embedding: databricks-bge-large-en (1024 dims)")
print(f"\n‚è≥ Initial sync in progress (5-15 minutes)")
print(f"üí° Monitor: Databricks UI ‚Üí ML ‚Üí Vector Search")
print(f"\nüìù Save these for retrieval:")
print(f"   ENDPOINT_NAME = '{ENDPOINT_NAME}'")
print(f"   INDEX_NAME = '{INDEX_NAME}'")

# COMMAND ----------

# ============================================================
# MONITOR INDEX SYNC STATUS
# ============================================================

from databricks.vector_search.client import VectorSearchClient
import time

vsc = VectorSearchClient(disable_notice=True)

ENDPOINT_NAME = 'cvip_endpoint'
INDEX_NAME = 'workspace.default.cvip_chunks_vs_index'

print("üîç Monitoring Index Sync Progress")
print("=" * 60)

# Function to check status
def check_index_status():
    try:
        index = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)
        index_info = index.describe()
        
        status = index_info.get('status', {})
        state = status.get('detailed_state', 'UNKNOWN')
        ready = status.get('ready', False)
        message = status.get('message', '')
        
        # Get indexing progress
        delta_sync = index_info.get('delta_sync_index_spec', {})
        
        return {
            'state': state,
            'ready': ready,
            'message': message,
            'index_info': index_info
        }
    except Exception as e:
        return {'error': str(e)}

# Check status immediately
print("üìä Current Status:")
status = check_index_status()

if 'error' in status:
    print(f"   ‚ö†Ô∏è Status check error: {status['error']}")
else:
    print(f"   State: {status['state']}")
    print(f"   Ready: {'‚úÖ YES' if status['ready'] else '‚è≥ NOT YET'}")
    print(f"   Message: {status['message']}")

# If not ready, monitor with periodic checks
if not status.get('ready', False):
    print("\n‚è≥ Index is syncing... This typically takes 5-15 minutes")
    print("   Checking every 60 seconds (will auto-stop when ready)")
    print("-" * 60)
    
    max_checks = 20  # Check for up to 20 minutes
    check_count = 0
    
    while check_count < max_checks:
        time.sleep(60)  # Wait 1 minute
        check_count += 1
        
        status = check_index_status()
        
        if 'error' in status:
            print(f"   [{check_count}] Error: {status['error']}")
            continue
        
        print(f"   [{check_count}] State: {status['state']} | Ready: {status['ready']}")
        
        if status['ready']:
            print("\nüéâ INDEX IS READY!")
            break
        
        if check_count % 5 == 0:
            print(f"      Still syncing... ({check_count} mins elapsed)")
    
    if not status.get('ready', False):
        print("\n‚è≥ Still syncing after 20 minutes")
        print("   This is normal for large datasets")
        print("   üí° You can proceed to other tasks and check back later")

else:
    print("\n‚úÖ Index is already READY!")

# Display final status details
print("\n" + "=" * 60)
print("üìã INDEX DETAILS")
print("=" * 60)

final_status = check_index_status()

if 'error' not in final_status:
    info = final_status['index_info']
    
    print(f"Name: {INDEX_NAME}")
    print(f"State: {final_status['state']}")
    print(f"Ready: {'‚úÖ YES' if final_status['ready'] else '‚è≥ NO'}")
    
    # Get configuration details
    delta_sync = info.get('delta_sync_index_spec', {})
    print(f"\nConfiguration:")
    print(f"  Source Table: {delta_sync.get('source_table', 'Unknown')}")
    print(f"  Primary Key: {delta_sync.get('primary_key', 'Unknown')}")
    print(f"  Embedding Column: {delta_sync.get('embedding_source_column', 'Unknown')}")
    print(f"  Embedding Model: {delta_sync.get('embedding_model_endpoint_name', 'Unknown')}")
    
    if final_status['ready']:
        print("\nüöÄ You can now proceed to test queries!")

# COMMAND ----------

# ============================================================
# FIXED: Manual workspace URL (no dynamic detection)
# ============================================================

from databricks.vector_search.client import VectorSearchClient

# Your workspace URL (from the browser or the sync message you saw earlier)
WORKSPACE_URL = "https://YOUR_WORKSPACE.cloud.databricks.com"

# Initialize with explicit URL
vsc = VectorSearchClient(
    workspace_url=WORKSPACE_URL,
    disable_notice=True
)

ENDPOINT_NAME = 'cvip_endpoint'
INDEX_NAME = 'workspace.default.cvip_chunks_vs_index'

print("üîç Checking Index Status (Fixed)")
print("=" * 60)

try:
    index = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)
    index_info = index.describe()
    
    status = index_info.get('status', {})
    state = status.get('detailed_state', 'UNKNOWN')
    ready = status.get('ready', False)
    message = status.get('message', 'None')
    
    print(f"üìä Index Status:")
    print(f"   State: {state}")
    print(f"   Ready: {'‚úÖ YES' if ready else '‚è≥ NO'}")
    print(f"   Message: {message[:150]}...")
    
    # Get delta sync details
    delta_sync = index_info.get('delta_sync_index_spec', {})
    
    print(f"\nüìã Configuration:")
    print(f"   Source: {delta_sync.get('source_table', 'Unknown')}")
    print(f"   Primary Key: {delta_sync.get('primary_key', 'Unknown')}")
    print(f"   Embedding Column: {delta_sync.get('embedding_source_column', 'Unknown')}")
    print(f"   Pipeline: {delta_sync.get('pipeline_type', 'Unknown')}")
    
    if ready:
        print("\n‚úÖ INDEX IS READY!")
        print("üéâ You can now run test queries!")
    else:
        print(f"\n‚è≥ Still syncing (state: {state})")
        print("   Continue waiting...")
        
        # Try to trigger sync again
        try:
            index.sync()
            print("   ‚úÖ Sync re-triggered")
        except Exception as sync_err:
            print(f"   ‚ÑπÔ∏è Sync note: {sync_err}")
    
except Exception as e:
    print(f"‚ùå Error: {e}")
    
    # If index doesn't exist, recreate
    if "does not exist" in str(e).lower() or "not found" in str(e).lower():
        print("\nüÜï Index not found - recreating...")
        
        try:
            vsc.create_delta_sync_index(
                endpoint_name=ENDPOINT_NAME,
                index_name=INDEX_NAME,
                source_table_name="workspace.default.cvip_chunks",
                pipeline_type="CONTINUOUS",
                primary_key="chunk_id",
                embedding_source_column="content",
                embedding_model_endpoint_name="databricks-bge-large-en"
            )
            print("‚úÖ Index recreated with CONTINUOUS pipeline")
        except Exception as create_err:
            print(f"‚ùå Recreation failed: {create_err}")

# COMMAND ----------

# ============================================================
# TEST VECTOR SEARCH - Comprehensive Test Suite
# ============================================================

from databricks.vector_search.client import VectorSearchClient

WORKSPACE_URL = "https://YOUR_WORKSPACE.cloud.databricks.com"
vsc = VectorSearchClient(workspace_url=WORKSPACE_URL, disable_notice=True)

ENDPOINT_NAME = 'cvip_endpoint'
INDEX_NAME = 'workspace.default.cvip_chunks_vs_index'

print("üß™ Testing Vector Search - Comprehensive Suite")
print("=" * 70)

# Get index
index = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)

# Test queries covering different topics
test_queries = [
    {
        'query': 'What is edge detection in image processing?',
        'expected': 'Should find chunks about edge detection, Sobel, Canny'
    },
    {
        'query': 'Explain convolutional neural networks',
        'expected': 'Should find chunks about CNNs, convolution, filters'
    },
    {
        'query': 'How does image segmentation work?',
        'expected': 'Should find chunks about segmentation algorithms'
    },
    {
        'query': 'What are vision transformers?',
        'expected': 'Should find chunks about ViT, attention mechanisms'
    }
]

for i, test_case in enumerate(test_queries, 1):
    query = test_case['query']
    
    print(f"\n{'='*70}")
    print(f"üîç Test {i}: {query}")
    print(f"Expected: {test_case['expected']}")
    print("-" * 70)
    
    try:
        results = index.similarity_search(
            query_text=query,
            columns=["chunk_id", "content", "citation_label", "page_number"],
            num_results=5
        )
        
        if results and 'result' in results and 'data_array' in results['result']:
            matches = results['result']['data_array']
            
            print(f"üìä Found {len(matches)} results:\n")
            
            for j, match in enumerate(matches, 1):
                chunk_id = match[0]
                content = match[1]
                citation = match[2]
                page = match[3]
                score = match[-1]
                
                print(f"   {j}. Score: {score:.4f} | Page: {page}")
                print(f"      {citation}")
                print(f"      {content[:150]}...")
                print()
        else:
            print("   ‚ö†Ô∏è No results returned")
            
    except Exception as e:
        print(f"   ‚ùå Search error: {e}")

print("=" * 70)
print("‚úÖ VECTOR SEARCH TESTS COMPLETE!")
print("=" * 70)

# COMMAND ----------

# ============================================================
# TEST ENHANCED RETRIEVAL
# ============================================================

print("üß™ Testing Enhanced Retrieval Pipeline")
print("=" * 70)

# Test query
test_query = "What are edge detection algorithms in image processing?"

print(f"üîç Query: {test_query}\n")

# For now, use basic retrieval (we'll add enhanced features next)
try:
    index = vsc.get_index(endpoint_name='cvip_endpoint', index_name='workspace.default.cvip_chunks_vs_index')
    
    results = index.similarity_search(
        query_text=test_query,
        columns=["chunk_id", "content", "citation_label", "page_number", "document_id"],
        num_results=10
    )
    
    if results and 'result' in results:
        matches = results['result']['data_array']
        
        print(f"üìä Retrieved {len(matches)} results:\n")
        
        # Add tier information from database
        chunk_ids = [m[0] for m in matches]
        chunk_ids_str = "', '".join(chunk_ids)
        
        tier_info = spark.sql(f"""
            SELECT 
                c.chunk_id,
                d.source_tier,
                d.priority_score,
                d.source_type
            FROM cvip_chunks c
            JOIN cvip_documents d ON c.document_id = d.document_id
            WHERE c.chunk_id IN ('{chunk_ids_str}')
        """).toPandas()
        
        tier_map = tier_info.set_index('chunk_id').to_dict('index')
        
        # Display with tier info
        for i, match in enumerate(matches[:5], 1):
            chunk_id = match[0]
            content = match[1]
            citation = match[2]
            page = match[3]
            score = match[-1]
            
            tier_data = tier_map.get(chunk_id, {})
            tier = tier_data.get('source_tier', '?')
            priority = tier_data.get('priority_score', 0)
            source_type = tier_data.get('source_type', 'unknown')
            
            print(f"{i}. [{source_type.upper()}] Tier {tier} | Priority: {priority}")
            print(f"   Similarity Score: {score:.4f}")
            print(f"   {citation}")
            print(f"   {content[:150]}...")
            print()
        
        print("‚úÖ Retrieval working with tier awareness!")
        
except Exception as e:
    print(f"‚ùå Error: {e}")

# COMMAND ----------

# ============================================================
# SAVE VECTOR SEARCH CONFIGURATION
# ============================================================

# Save these constants for use throughout your project
VECTOR_SEARCH_CONFIG = {
    'workspace_url': 'https://YOUR_WORKSPACE.cloud.databricks.com',
    'endpoint_name': 'cvip_endpoint',
    'index_name': 'workspace.default.cvip_chunks_vs_index',
    'catalog': 'workspace',
    'schema': 'default',
    'embedding_model': 'databricks-bge-large-en',
    'embedding_dimension': 1024
}

print("‚úÖ Vector Search Configuration Saved:")
print(f"   Workspace: {VECTOR_SEARCH_CONFIG['workspace_url']}")
print(f"   Endpoint: {VECTOR_SEARCH_CONFIG['endpoint_name']}")
print(f"   Index: {VECTOR_SEARCH_CONFIG['index_name']}")
print(f"   Embedding: {VECTOR_SEARCH_CONFIG['embedding_model']} ({VECTOR_SEARCH_CONFIG['embedding_dimension']} dims)")

# Test quick access
from databricks.vector_search.client import VectorSearchClient

def get_vector_index():
    """Helper function to get vector index"""
    vsc = VectorSearchClient(
        workspace_url=VECTOR_SEARCH_CONFIG['workspace_url'],
        disable_notice=True
    )
    return vsc.get_index(
        endpoint_name=VECTOR_SEARCH_CONFIG['endpoint_name'],
        index_name=VECTOR_SEARCH_CONFIG['index_name']
    )

# Test it
test_index = get_vector_index()
print("\n‚úÖ Quick access helper working!")

# COMMAND ----------

# ============================================================
# PRODUCTION RAG SYSTEM FOR CVIP - COMPLETE IMPLEMENTATION
# ============================================================

from databricks.vector_search.client import VectorSearchClient
from typing import List, Dict, Optional, Tuple
import re
from datetime import datetime
from collections import deque
import uuid
import json

# ============================================================
# CONFIGURATION
# ============================================================

class CVIPConfig:
    """Centralized configuration for CVIP RAG system"""
    
    # Vector Search Configuration
    WORKSPACE_URL = "https://YOUR_WORKSPACE.cloud.databricks.com"
    ENDPOINT_NAME = "cvip_endpoint"
    INDEX_NAME = "workspace.default.cvip_chunks_vs_index"
    
    # LLM Configuration
    # Using Claude Sonnet via Databricks Foundation Models
    # Alternative: "databricks-meta-llama-3-1-70b-instruct" or "databricks-dbrx-instruct"
    LLM_ENDPOINT = "databricks-meta-llama-3-1-70b-instruct"  # High quality, fast
    LLM_TEMPERATURE = 0.1  # Low for factual accuracy
    LLM_MAX_TOKENS = 800
    
    # Retrieval Configuration
    RETRIEVAL_TOP_K = 15  # Get more candidates for reranking
    FINAL_CONTEXT_K = 5   # Use top 5 after reranking
    
    # Scoring Weights (intent-aware)
    SCORING_WEIGHTS = {
        'foundational': {'alpha': 0.6, 'beta': 0.4},  # Boost textbooks
        'advanced': {'alpha': 0.8, 'beta': 0.2},      # Prioritize relevance
        'comparison': {'alpha': 0.7, 'beta': 0.3},    # Balanced
        'general': {'alpha': 0.7, 'beta': 0.3}        # Default
    }
    
    # Conversation Configuration
    MAX_MEMORY_TURNS = 10  # Last 10 exchanges
    SESSION_TIMEOUT_HOURS = 24
    
    # Answer Grounding Thresholds
    MIN_SUPPORT_SCORE = 0.6  # Minimum overlap for "supported"
    HALLUCINATION_WARNING_THRESHOLD = 0.4


print("‚úÖ Configuration loaded")

# COMMAND ----------

# ============================================================
# PRODUCTION RAG SYSTEM - ALL SECURITY & PERFORMANCE FIXES
# ============================================================

import os
import re
import json
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import deque
import threading

# ============================================================
# SECURE CONFIGURATION WITH ENVIRONMENT VARIABLES
# ============================================================

class SecureConfig:
    """Secure configuration using environment variables and secrets"""
    
    def __init__(self):
        # FIXED: Use environment variables / secrets instead of hardcoded values
        self.WORKSPACE_URL = self._get_secret("WORKSPACE_URL", 
            default="https://YOUR_WORKSPACE.cloud.databricks.com")
        self.ENDPOINT_NAME = self._get_secret("VECTOR_ENDPOINT", 
            default="cvip_endpoint")
        self.INDEX_NAME = self._get_secret("VECTOR_INDEX", 
            default="workspace.default.cvip_chunks_vs_index")
        
        # LLM Configuration
        self.LLM_ENDPOINT = os.getenv("LLM_ENDPOINT", "databricks-meta-llama-3-1-70b-instruct")
        self.LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.1"))
        self.LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "800"))
        
        # Retrieval Configuration
        self.RETRIEVAL_TOP_K = 15
        self.FINAL_CONTEXT_K = 5
        
        # Scoring Weights
        self.SCORING_WEIGHTS = {
            'foundational': {'alpha': 0.6, 'beta': 0.4},
            'advanced': {'alpha': 0.8, 'beta': 0.2},
            'comparison': {'alpha': 0.7, 'beta': 0.3},
            'general': {'alpha': 0.7, 'beta': 0.3}
        }
        
        # Answer Grounding
        self.MIN_SUPPORT_SCORE = 0.6
        self.HALLUCINATION_WARNING_THRESHOLD = 0.4
        
        # Cache Configuration
        self.ENABLE_METADATA_CACHE = True
        self.CACHE_TTL_SECONDS = 3600  # 1 hour
    
    def _get_secret(self, key: str, default: str = None) -> str:
        """
        Get secret from Databricks secrets or environment variable.
        FIXED: Proper secret management
        """
        try:
            # Try Databricks secrets first
            from databricks.sdk.runtime import dbutils
            scope = os.getenv("SECRET_SCOPE", "cvip-secrets")
            return dbutils.secrets.get(scope=scope, key=key)
        except:
            # Fallback to environment variable
            value = os.getenv(key, default)
            if value is None:
                raise ValueError(f"Required configuration {key} not found")
            return value
    
    def validate(self):
        """Validate configuration at startup"""
        required = ['WORKSPACE_URL', 'ENDPOINT_NAME', 'INDEX_NAME']
        for attr in required:
            if not getattr(self, attr):
                raise ValueError(f"Missing required configuration: {attr}")
        print("‚úÖ Configuration validated")


# ============================================================
# METADATA CACHE - PERFORMANCE FIX #3
# ============================================================

class MetadataCache:
    """
    FIXED: In-memory cache for tier metadata to avoid Pandas conversion
    on every query (Issue #3)
    """
    
    def __init__(self, ttl_seconds: int = 3600):
        self._cache: Dict[str, Dict] = {}
        self._cache_time: Optional[datetime] = None
        self._lock = threading.Lock()
        self.ttl_seconds = ttl_seconds
    
    def get(self, chunk_id: str) -> Optional[Dict]:
        """Get metadata for chunk_id"""
        if self._is_expired():
            return None
        return self._cache.get(chunk_id)
    
    def bulk_get(self, chunk_ids: List[str]) -> Dict[str, Dict]:
        """Get metadata for multiple chunk_ids"""
        if self._is_expired():
            return {}
        return {cid: self._cache[cid] for cid in chunk_ids if cid in self._cache}
    
    def update(self, metadata_dict: Dict[str, Dict]):
        """Update cache with new metadata"""
        with self._lock:
            self._cache.update(metadata_dict)
            self._cache_time = datetime.now()
    
    def _is_expired(self) -> bool:
        """Check if cache is expired"""
        if self._cache_time is None:
            return True
        elapsed = (datetime.now() - self._cache_time).total_seconds()
        return elapsed > self.ttl_seconds
    
    def clear(self):
        """Clear cache"""
        with self._lock:
            self._cache.clear()
            self._cache_time = None


# ============================================================
# SAFE METADATA FETCHER - SQL INJECTION FIX #2
# ============================================================

class SafeMetadataFetcher:
    """
    FIXED: SQL injection-safe metadata fetching using DataFrame joins
    instead of string interpolation (Issue #2)
    """
    
    def __init__(self, cache: MetadataCache):
        self.cache = cache
        # Verify Spark is available
        try:
            from pyspark.sql import SparkSession
            self.spark = SparkSession.getActiveSession()
            if self.spark is None:
                raise RuntimeError("No active Spark session found")
        except ImportError:
            raise RuntimeError("PySpark not available. This system requires Databricks environment.")
    
    def fetch_metadata(self, chunk_ids: List[str]) -> Dict[str, Dict]:
        """
        FIXED: Safe metadata fetching using DataFrame operations.
        No SQL injection risk.
        """
        if not chunk_ids:
            return {}
        
        # Check cache first
        cached = self.cache.bulk_get(chunk_ids)
        
        # Find missing IDs
        missing_ids = [cid for cid in chunk_ids if cid not in cached]
        
        if not missing_ids:
            return cached
        
        # FIXED: Use DataFrame join instead of string concatenation
        from pyspark.sql.functions import col
        
        # Create DataFrame from chunk_ids
        chunk_ids_df = self.spark.createDataFrame(
            [(cid,) for cid in missing_ids],
            ["chunk_id"]
        )
        
        # Read tables
        chunks_df = self.spark.table("cvip_chunks")
        documents_df = self.spark.table("cvip_documents")
        
        # Safe join operation
        metadata_df = (
            chunk_ids_df
            .join(chunks_df, "chunk_id", "left")
            .join(documents_df, chunks_df.document_id == documents_df.document_id, "left")
            .select(
                chunks_df.chunk_id,
                documents_df.source_tier,
                documents_df.priority_score,
                documents_df.source_type,
                documents_df.source_name
            )
        )
        
        # Convert to dict (only for missing IDs, not all queries)
        metadata_rows = metadata_df.collect()
        
        new_metadata = {}
        for row in metadata_rows:
            new_metadata[row.chunk_id] = {
                'source_tier': row.source_tier,
                'priority_score': row.priority_score,
                'source_type': row.source_type,
                'source_name': row.source_name
            }
        
        # Update cache
        self.cache.update(new_metadata)
        
        # Merge with cached data
        result = {**cached, **new_metadata}
        return result


# ============================================================
# SAFE RESULT PARSER - FRAGILITY FIX #4
# ============================================================

class SafeResultParser:
    """
    FIXED: Robust result parsing with explicit schema handling (Issue #4)
    """
    
    @staticmethod
    def parse_vector_search_results(results: Dict, expected_columns: List[str]) -> List[Dict]:
        """
        FIXED: Parse results with explicit column mapping.
        Doesn't assume position or rely on [-1] indexing.
        """
        chunks = []
        
        if not results or 'result' not in results:
            return chunks
        
        result_data = results['result']
        
        # Get column names if available
        column_names = result_data.get('columns', expected_columns)
        data_array = result_data.get('data_array', [])
        
        # Build column index map
        column_map = {name: idx for idx, name in enumerate(column_names)}
        
        # Parse each result row safely
        for row in data_array:
            try:
                chunk = {}
                
                # Map columns explicitly
                for col_name in expected_columns:
                    idx = column_map.get(col_name)
                    if idx is not None and idx < len(row):
                        chunk[col_name] = row[idx]
                    else:
                        chunk[col_name] = None
                
                # Score is typically last, but get it by name if possible
                if 'score' in column_map:
                    chunk['similarity_score'] = row[column_map['score']]
                elif len(row) > len(expected_columns):
                    # Fallback: assume last element is score
                    chunk['similarity_score'] = row[-1]
                else:
                    chunk['similarity_score'] = 0.0
                
                chunks.append(chunk)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Error parsing result row: {e}")
                continue
        
        return chunks


# ============================================================
# CROSS-ENCODER MANAGER - LOADING FIX #5
# ============================================================

class CrossEncoderManager:
    """
    FIXED: Load cross-encoder once at startup, not per request (Issue #5)
    """
    
    def __init__(self, enabled: bool = False):
        self.enabled = enabled
        self.model = None
        self._lock = threading.Lock()
        
        if enabled:
            self._load_model()
    
    def _load_model(self):
        """Load cross-encoder model (once at startup)"""
        try:
            from sentence_transformers import CrossEncoder
            with self._lock:
                if self.model is None:
                    print("üîÑ Loading cross-encoder model...")
                    self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
                    print("‚úÖ Cross-encoder loaded")
        except ImportError:
            print("‚ö†Ô∏è sentence-transformers not available. Reranking disabled.")
            self.enabled = False
    
    def rerank(self, query: str, chunks: List[Dict]) -> List[Dict]:
        """Rerank chunks using cross-encoder"""
        if not self.enabled or self.model is None:
            # Fallback: just return chunks as-is
            for chunk in chunks:
                chunk['final_score'] = chunk.get('weighted_score', 0.0)
            return chunks
        
        try:
            pairs = [[query, chunk['content']] for chunk in chunks]
            scores = self.model.predict(pairs)
            
            for i, chunk in enumerate(chunks):
                chunk['ce_score'] = float(scores[i])
                chunk['final_score'] = (
                    0.7 * chunk.get('weighted_score', 0.0) + 
                    0.3 * chunk['ce_score']
                )
            
            chunks.sort(key=lambda x: x['final_score'], reverse=True)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Reranking error: {e}")
            for chunk in chunks:
                chunk['final_score'] = chunk.get('weighted_score', 0.0)
        
        return chunks


# ============================================================
# IMPROVED GROUNDING CHECKER - HEURISTIC FIX #6
# ============================================================

class ImprovedGroundingChecker:
    """
    FIXED: Better grounding verification using multiple signals (Issue #6)
    """
    
    @staticmethod
    def verify_grounding(
        answer: str,
        retrieved_chunks: List[Dict],
        query: str,
        config: SecureConfig
    ) -> Dict:
        """
        Multi-signal grounding verification:
        1. Token overlap (baseline)
        2. Citation presence
        3. Factual claim extraction
        4. Context alignment
        """
        
        # Signal 1: Token overlap (existing method, but improved)
        overlap_score = ImprovedGroundingChecker._calculate_overlap(answer, retrieved_chunks)
        
        # Signal 2: Citation presence
        citation_score = ImprovedGroundingChecker._check_citations(answer, retrieved_chunks)
        
        # Signal 3: Sentence-level alignment
        alignment_score = ImprovedGroundingChecker._check_sentence_alignment(answer, retrieved_chunks)
        
        # Weighted combination
        final_score = (
            0.4 * overlap_score +
            0.3 * citation_score +
            0.3 * alignment_score
        )
        
        # Classify support level
        if final_score >= config.MIN_SUPPORT_SCORE:
            support_level = 'fully_supported'
        elif final_score >= config.HALLUCINATION_WARNING_THRESHOLD:
            support_level = 'partially_supported'
        else:
            support_level = 'not_supported'
        
        # Extract citations
        citation_pattern = r'\[Source:([^\]]+)\]'
        citations = re.findall(citation_pattern, answer)
        
        # Get used chunk IDs
        used_chunk_ids = []
        for chunk in retrieved_chunks[:5]:
            if chunk['citation_label'] in answer or chunk['source_name'] in answer:
                used_chunk_ids.append(chunk['chunk_id'])
        
        return {
            'citations': citations,
            'support_level': support_level,
            'confidence': final_score,
            'used_chunk_ids': used_chunk_ids,
            'scores': {
                'overlap': overlap_score,
                'citation': citation_score,
                'alignment': alignment_score
            }
        }
    
    @staticmethod
    def _calculate_overlap(answer: str, chunks: List[Dict]) -> float:
        """Calculate token overlap between answer and context"""
        answer_terms = set(re.findall(r'\b\w{4,}\b', answer.lower()))
        
        context_terms = set()
        for chunk in chunks:
            context_terms.update(re.findall(r'\b\w{4,}\b', chunk['content'].lower()))
        
        if not answer_terms:
            return 0.0
        
        overlap = len(answer_terms & context_terms)
        return overlap / len(answer_terms)
    
    @staticmethod
    def _check_citations(answer: str, chunks: List[Dict]) -> float:
        """Check if answer contains proper citations"""
        citation_pattern = r'\[Source:'
        citations = re.findall(citation_pattern, answer)
        
        # Score based on citation presence and density
        if not citations:
            return 0.0
        
        # Normalize by answer length (expect ~1 citation per 100 words)
        word_count = len(answer.split())
        expected_citations = max(1, word_count // 100)
        
        citation_ratio = min(len(citations) / expected_citations, 1.0)
        return citation_ratio
    
    @staticmethod
    def _check_sentence_alignment(answer: str, chunks: List[Dict]) -> float:
        """Check if answer sentences align with context"""
        # Split answer into sentences
        sentences = re.split(r'[.!?]+', answer)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if not sentences:
            return 0.0
        
        # Check each sentence for alignment
        aligned_count = 0
        context_text = " ".join(chunk['content'] for chunk in chunks)
        context_lower = context_text.lower()
        
        for sentence in sentences:
            # Extract key phrases from sentence
            key_phrases = re.findall(r'\b\w{5,}\b', sentence.lower())
            
            # Check if phrase appears in context
            if any(phrase in context_lower for phrase in key_phrases):
                aligned_count += 1
        
        return aligned_count / len(sentences)


print("‚úÖ All security and performance fixes loaded")

# COMMAND ----------

# ============================================================
# FIXED ANSWER GENERATOR - SYSTEM PROMPT BUG FIX #7
# ============================================================

class FixedAnswerGenerator:
    """
    FIXED: Proper system prompt formatting and LLM error handling (Issue #7, #8)
    """
    
    def __init__(self, config: SecureConfig):
        self.config = config
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """
        FIXED: Explicit dependency check and fallback (Issue #8)
        """
        try:
            from databricks_genai_inference import ChatSession
            
            # FIXED: Format current_date properly in system prompt
            current_date = datetime.now().strftime("%B %d, %Y")
            system_prompt = self._get_system_prompt(current_date)
            
            self.llm = ChatSession(
                model=self.config.LLM_ENDPOINT,
                system_message=system_prompt,
                max_tokens=self.config.LLM_MAX_TOKENS,
                temperature=self.config.LLM_TEMPERATURE
            )
            
            print(f"‚úÖ LLM initialized: {self.config.LLM_ENDPOINT}")
            
        except ImportError as e:
            raise RuntimeError(
                "databricks_genai_inference not available. "
                "This system requires Databricks ML runtime with GenAI support. "
                f"Error: {e}"
            )
        except Exception as e:
            raise RuntimeError(f"Failed to initialize LLM: {e}")
    
    def _get_system_prompt(self, current_date: str) -> str:
        """
        FIXED: Properly formatted system prompt with date (Issue #7)
        """
        return f"""You are an expert AI assistant specializing in Computer Vision and Image Processing.

Current Date: {current_date}

CRITICAL RULES:
1. Answer ONLY using information from the provided CONTEXT
2. If the context doesn't contain the answer, say "I don't have enough information in my knowledge base to answer this question."
3. Always cite your sources using the provided citation labels
4. Be precise and technical when appropriate
5. Never make up information or speculate beyond the context
6. Do not mention knowledge cutoffs or limitations unless directly relevant

When answering:
- Start with a direct, clear answer
- Provide technical details and explanations from the context
- Include relevant equations, algorithms, or formulas if mentioned in context
- Cite sources using format: [Source: <citation_label>]
- If multiple sources support your answer, cite all relevant ones

Remember: Accuracy and groundedness are more important than comprehensiveness. If unsure, say so."""
    
    def generate(
        self,
        query: str,
        retrieved_chunks: List[Dict],
        query_classification: Dict
    ) -> Dict:
        """Generate answer with improved error handling"""
        
        if not retrieved_chunks:
            return {
                'answer': "I couldn't find relevant information in my knowledge base to answer your question.",
                'citations': [],
                'support_level': 'not_supported',
                'confidence': 0.0,
                'used_chunks': [],
                'error': None
            }
        
        # Build context
        context = self._build_context(retrieved_chunks)
        
        # Create prompt
        prompt = self._create_prompt(query, context, query_classification)
        
        # Generate with retry logic
        max_retries = 2
        for attempt in range(max_retries):
            try:
                response = self.llm.reply(prompt)
                
                # Handle different response formats
                if isinstance(response, dict):
                    answer_text = response.get('message', str(response))
                else:
                    answer_text = str(response)
                
                break
                
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"‚ö†Ô∏è Generation attempt {attempt + 1} failed, retrying...")
                    continue
                else:
                    print(f"‚ùå Generation failed after {max_retries} attempts: {e}")
                    return {
                        'answer': f"I encountered an error generating the answer. Please try rephrasing your question.",
                        'citations': [],
                        'support_level': 'not_supported',
                        'confidence': 0.0,
                        'used_chunks': [],
                        'error': str(e)
                    }
        
        # Verify grounding using improved checker
        grounding_result = ImprovedGroundingChecker.verify_grounding(
            answer_text,
            retrieved_chunks,
            query,
            self.config
        )
        
        return {
            'answer': answer_text,
            'citations': grounding_result['citations'],
            'support_level': grounding_result['support_level'],
            'confidence': grounding_result['confidence'],
            'used_chunks': grounding_result['used_chunk_ids'],
            'grounding_scores': grounding_result['scores'],
            'error': None
        }
    
    def _build_context(self, chunks: List[Dict]) -> str:
        """Build formatted context from chunks"""
        context_parts = []
        
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(
                f"[SOURCE {i}] {chunk['citation_label']}\n"
                f"Content: {chunk['content']}\n"
            )
        
        return "\n---\n".join(context_parts)
    
    def _create_prompt(
        self,
        query: str,
        context: str,
        query_classification: Dict
    ) -> str:
        """Create well-structured prompt"""
        
        prompt = f"""CONTEXT (from trusted Computer Vision and Image Processing sources):

{context}

---

USER QUESTION: {query}

INSTRUCTIONS:
- Answer using ONLY the information from the CONTEXT above
- Be technically accurate and detailed
- Cite sources using [Source: <citation>] format
- If information is not in the context, explicitly state: "I don't have this information in my knowledge base"
- Do not make assumptions or speculate

YOUR ANSWER:"""
        
        return prompt


print("‚úÖ Fixed Answer Generator ready")

# COMMAND ----------

# ============================================================
# FINAL PRODUCTION MEMORY SYSTEM - ALL CORRECTIONS
# ============================================================

from collections import deque
from typing import List, Dict, Optional
import uuid
import json
import re
import logging
from datetime import datetime
import tiktoken

logger = logging.getLogger(__name__)

class ProductionMemorySystem:
    """
    Final production memory system with all corrections applied.
    
    Corrections:
    - Fixed recency calculation (age-based, not insertion-based)
    - Fixed restore_from_persistence window logic
    - Enhanced PII redaction (API keys, secrets)
    - Improved summary quality
    """
    
    def __init__(
        self,
        max_window_size: int = 10,
        max_context_tokens: int = 2000,
        enable_persistence: bool = False
    ):
        self.max_window_size = max_window_size
        self.max_context_tokens = max_context_tokens
        self.enable_persistence = enable_persistence
        
        # Memory structures
        self.sliding_window = deque(maxlen=max_window_size)
        self.full_session_log: List[Dict] = []
        self.conversation_summary: Optional[str] = None
        
        # Session metadata
        self.session_id = str(uuid.uuid4())
        self.session_start = datetime.now()
        self.turn_count = 0
        
        # Tokenizer
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except:
            logger.warning("Tiktoken not available, using estimation")
            self.tokenizer = None
        
        # Salience decay factor
        self.recency_decay = 0.9
    
    def add_turn(
        self,
        query: str,
        answer: str,
        metadata: Dict
    ) -> Dict:
        """Add turn with enhanced PII redaction"""
        self.turn_count += 1
        
        # FIXED: Enhanced PII redaction
        redacted_query = self._redact_pii(query)
        redacted_answer = self._redact_pii(answer)
        
        turn_entry = {
            'turn_id': str(uuid.uuid4()),
            'turn_number': self.turn_count,
            'session_id': self.session_id,
            'timestamp': datetime.now().isoformat(),
            'query': redacted_query,
            'answer': redacted_answer,
            'metadata': metadata,
            'base_salience': self._calculate_base_salience(metadata)
        }
        
        self.sliding_window.append(turn_entry)
        self.full_session_log.append(turn_entry)
        
        if self.enable_persistence:
            self._persist_turn(turn_entry)
        
        if self.turn_count % 10 == 0:
            self._create_conversation_summary()
        
        return turn_entry
    
    def _calculate_base_salience(self, metadata: Dict) -> float:
        """
        Calculate base salience (without recency).
        FIXED: Recency calculated dynamically when selecting context.
        """
        confidence = metadata.get('confidence', 0.5)
        
        support_level = metadata.get('support_level', 'not_supported')
        support_boost = {
            'fully_supported': 1.0,
            'partially_supported': 0.7,
            'not_supported': 0.3,
            'memory_recall': 0.5,
            'out_of_domain': 0.2
        }.get(support_level, 0.5)
        
        # Base salience (without recency)
        return 0.5 * confidence + 0.5 * support_boost
    
    def _calculate_dynamic_salience(self, turn: Dict) -> float:
        """
        FIXED: Calculate salience with age-based recency.
        
        Recency based on turn age, not insertion time.
        """
        base_salience = turn['base_salience']
        
        # Calculate age (how many turns ago)
        age = self.turn_count - turn['turn_number']
        
        # Recency decays with age
        recency_factor = self.recency_decay ** age
        
        # Combine base salience with recency
        return 0.7 * base_salience + 0.3 * recency_factor
    
    def _redact_pii(self, text: str) -> str:
        """
        FIXED: Enhanced PII redaction with API keys and secrets.
        """
        # Email addresses
        text = re.sub(
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            '[EMAIL]',
            text
        )
        
        # Phone numbers (multiple formats)
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '[PHONE]', text)
        text = re.sub(r'\b\(\d{3}\)\s*\d{3}[-.]?\d{4}\b', '[PHONE]', text)
        
        # Credit card patterns
        text = re.sub(
            r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b',
            '[CARD]',
            text
        )
        
        # FIXED: API keys and secrets (common patterns)
        # AWS keys
        text = re.sub(r'\bAKIA[0-9A-Z]{16}\b', '[AWS_KEY]', text)
        
        # Generic API keys (long alphanumeric strings)
        text = re.sub(
            r'\b[A-Za-z0-9_-]{32,}\b',
            '[API_KEY]',
            text,
            flags=re.IGNORECASE
        )
        
        # Bearer tokens
        text = re.sub(
            r'Bearer\s+[A-Za-z0-9._-]+',
            'Bearer [TOKEN]',
            text,
            flags=re.IGNORECASE
        )
        
        # Social Security Numbers (US)
        text = re.sub(r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]', text)
        
        return text
    
    def get_context_for_llm(self, current_query: str) -> str:
        """
        FIXED: Uses dynamic salience calculation with proper recency.
        """
        if not self.sliding_window:
            return ""
        
        context_parts = []
        token_budget = self.max_context_tokens
        
        # Add conversation summary if available
        if self.conversation_summary:
            summary_tokens = self._count_tokens(self.conversation_summary)
            if summary_tokens < token_budget * 0.3:
                context_parts.append(f"CONVERSATION SUMMARY:\n{self.conversation_summary}\n")
                token_budget -= summary_tokens
        
        context_parts.append("RECENT CONVERSATION:")
        
        # FIXED: Calculate dynamic salience for each turn
        turns_with_salience = []
        for turn in self.sliding_window:
            dynamic_salience = self._calculate_dynamic_salience(turn)
            turns_with_salience.append((turn, dynamic_salience))
        
        # Sort by dynamic salience
        turns_with_salience.sort(key=lambda x: x[1], reverse=True)
        
        # Add turns within token budget
        for turn, salience in turns_with_salience:
            turn_text = (
                f"\nTurn {turn['turn_number']}:\n"
                f"User: {turn['query']}\n"
                f"Assistant: {turn['answer'][:500]}...\n"
            )
            
            turn_tokens = self._count_tokens(turn_text)
            
            if token_budget - turn_tokens > 100:
                context_parts.append(turn_text)
                token_budget -= turn_tokens
            else:
                break
        
        return "\n".join(context_parts)
    
    def _count_tokens(self, text: str) -> int:
        """Count tokens"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text) // 4
    
    def _create_conversation_summary(self):
        """
        FIXED: Improved summary with noun phrase extraction.
        
        Phase 1: Simple keyword extraction
        Phase 1.5: Can upgrade to LLM-based summarization
        """
        if len(self.full_session_log) < 5:
            return
        
        turns_to_summarize = self.full_session_log[:-self.max_window_size]
        
        if not turns_to_summarize:
            return
        
        # Extract domain-relevant topics
        topics = []
        for turn in turns_to_summarize:
            if turn['metadata'].get('is_domain_relevant'):
                query = turn['query']
                
                # FIXED: Better keyword extraction using noun phrases
                # Extract capitalized words (proper nouns) and technical terms
                capitalized = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', query)
                technical = re.findall(r'\b(?:CNN|ResNet|YOLO|SIFT|edge|detection|segmentation|filter|convolution|neural|network|transform|feature)\b', query, re.IGNORECASE)
                
                topics.extend(capitalized[:2])
                topics.extend(technical[:2])
        
        # Deduplicate and limit
        unique_topics = []
        seen = set()
        for topic in topics:
            topic_lower = topic.lower()
            if topic_lower not in seen:
                unique_topics.append(topic)
                seen.add(topic_lower)
                if len(unique_topics) >= 10:
                    break
        
        if unique_topics:
            self.conversation_summary = (
                f"Earlier discussion topics: {', '.join(unique_topics)}. "
                f"({len(turns_to_summarize)} turns summarized)"
            )
            logger.info(f"Created summary: {len(turns_to_summarize)} turns, {len(unique_topics)} topics")
    
    def restore_from_persistence(self, session_id: str) -> bool:
        """
        FIXED: Proper window reconstruction after restore.
        """
        try:
            from pyspark.sql import SparkSession
            from pyspark.sql.functions import col
            
            spark = SparkSession.getActiveSession()
            
            if not spark:
                return False
            
            # Load session logs
            logs_df = spark.table("cvip_query_logs").filter(
                col("session_id") == session_id
            ).orderBy("created_at")
            
            logs = logs_df.collect()
            
            if not logs:
                return False
            
            # FIXED: Restore all to full log first
            self.full_session_log.clear()
            
            for log in logs:
                turn_entry = {
                    'turn_id': log.query_id,
                    'turn_number': len(self.full_session_log) + 1,
                    'session_id': log.session_id,
                    'timestamp': log.created_at.isoformat(),
                    'query': log.user_query,
                    'answer': log.generated_answer or "",
                    'metadata': {
                        'support_level': log.support_label,
                        'confidence': log.confidence_score or 0.0
                    },
                    'base_salience': log.confidence_score or 0.5
                }
                
                self.full_session_log.append(turn_entry)
            
            # FIXED: Rebuild sliding window from last N turns
            self.sliding_window.clear()
            last_n_turns = self.full_session_log[-self.max_window_size:]
            for turn in last_n_turns:
                self.sliding_window.append(turn)
            
            self.session_id = session_id
            self.turn_count = len(self.full_session_log)
            
            logger.info(f"Restored session {session_id}: {self.turn_count} turns, window size {len(self.sliding_window)}")
            return True
            
        except Exception as e:
            logger.error(f"Restoration failed: {e}")
            return False
    
    def recall_by_ordinal(self, ordinal: str) -> Optional[Dict]:
        """Ordinal recall"""
        ordinal_map = {
            'first': 0, '1st': 0,
            'second': 1, '2nd': 1,
            'third': 2, '3rd': 2,
            'fourth': 3, '4th': 3,
            'fifth': 4, '5th': 4,
            'last': -1, 'latest': -1, 'previous': -1
        }
        
        index = ordinal_map.get(ordinal.lower())
        if index is None:
            return None
        
        try:
            return self.full_session_log[index]
        except IndexError:
            return None
    
    def search_turns(self, keyword: str) -> List[Dict]:
        """Search with result limiting"""
        keyword_lower = keyword.lower()
        matches = []
        max_matches = 20
        
        for turn in self.full_session_log:
            if len(matches) >= max_matches:
                break
            
            if (keyword_lower in turn['query'].lower() or 
                keyword_lower in turn['answer'][:500].lower()):
                matches.append(turn)
        
        return matches
    
    def _persist_turn(self, turn_entry: Dict):
        """Persistence handled by QueryLogger"""
        pass
    
    def get_session_summary(self) -> Dict:
        """Session summary"""
        duration = datetime.now() - self.session_start
        
        # Calculate average base salience
        avg_salience = (
            sum(t['base_salience'] for t in self.full_session_log) / len(self.full_session_log)
            if self.full_session_log else 0.0
        )
        
        domain_queries = sum(
            1 for turn in self.full_session_log 
            if turn['metadata'].get('is_domain_relevant', False)
        )
        
        memory_queries = sum(
            1 for turn in self.full_session_log
            if turn['metadata'].get('is_memory_query', False)
        )
        
        return {
            'session_id': self.session_id,
            'session_start': self.session_start.isoformat(),
            'session_duration': str(duration).split('.')[0],
            'total_turns': self.turn_count,
            'domain_queries': domain_queries,
            'memory_queries': memory_queries,
            'general_queries': self.turn_count - domain_queries - memory_queries,
            'avg_salience': round(avg_salience, 3),
            'has_summary': self.conversation_summary is not None
        }


print("‚úÖ Final Production Memory System ready")

# COMMAND ----------

# ============================================================
# FINAL PRODUCTION QUERY LOGGER - ALL CORRECTIONS
# ============================================================

import atexit
import logging
import uuid  # FIXED: Missing import
from typing import List, Dict, Optional
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, TimestampType, BooleanType, ArrayType
)
from datetime import datetime
import time

logger = logging.getLogger(__name__)

class FinalQueryLogger:
    """
    Final production query logger with all corrections.
    
    Corrections:
    - Added missing uuid import
    - Flush lifecycle improvements (explicit + atexit)
    """
    
    LOG_SCHEMA = StructType([
        StructField("query_id", StringType(), False),
        StructField("session_id", StringType(), False),
        StructField("user_query", StringType(), False),
        StructField("query_type", StringType(), True),
        StructField("routed_to_retrieval", BooleanType(), True),
        StructField("top_k", IntegerType(), True),
        StructField("retrieved_chunk_ids", ArrayType(StringType()), True),
        StructField("retrieved_scores", ArrayType(DoubleType()), True),
        StructField("generated_answer", StringType(), True),
        StructField("support_label", StringType(), True),
        StructField("confidence_score", DoubleType(), True),
        StructField("latency_ms", IntegerType(), True),
        StructField("model_name", StringType(), True),
        StructField("user_feedback", StringType(), True),
        StructField("feedback_comment", StringType(), True),
        StructField("error_message", StringType(), True),
        StructField("created_at", TimestampType(), False)
    ])
    
    def __init__(self, config):
        self.config = config
        self.batch_buffer: List[Dict] = []
        self.batch_size = 10
        
        # Fully qualified table name
        catalog = getattr(config, 'CATALOG', 'workspace')
        schema = getattr(config, 'SCHEMA', 'default')
        self.table_name = f"{catalog}.{schema}.cvip_query_logs"
        
        self.spark = None
        self._initialize_spark()
        
        if self.spark:
            atexit.register(self._flush_on_exit)
            logger.info(f"Query logger initialized: {self.table_name}")
    
    def _initialize_spark(self):
        """Initialize Spark with validation"""
        try:
            self.spark = SparkSession.getActiveSession()
            
            if self.spark is None:
                logger.warning("No Spark session. Logging disabled.")
            else:
                try:
                    self.spark.table(self.table_name)
                    logger.info(f"Verified table: {self.table_name}")
                except:
                    logger.warning(f"Table {self.table_name} not found. Creating...")
                    self._create_table_if_not_exists()
        except Exception as e:
            logger.error(f"Spark init failed: {e}")
            self.spark = None
    
    def _create_table_if_not_exists(self):
        """Create table if needed"""
        try:
            empty_df = self.spark.createDataFrame([], schema=self.LOG_SCHEMA)
            empty_df.write.format("delta").mode("ignore").saveAsTable(self.table_name)
            logger.info(f"Created table: {self.table_name}")
        except Exception as e:
            logger.error(f"Table creation failed: {e}")
    
    def log_query(
        self,
        session_id: str,
        query: str,
        query_classification: Dict,
        retrieved_chunks: List[Dict],
        generation_result: Dict,
        latency_ms: int,
        error: Optional[str] = None
    ):
        """Log query with validation"""
        if self.spark is None:
            return
        
        try:
            log_entry = {
                'query_id': str(uuid.uuid4()),
                'session_id': session_id,
                'user_query': query[:1000],
                'query_type': self._classify_query_type(query_classification),
                'routed_to_retrieval': query_classification.get('is_domain_relevant', False),
                'top_k': len(retrieved_chunks),
                'retrieved_chunk_ids': [c.get('chunk_id', '') for c in retrieved_chunks],
                'retrieved_scores': [float(c.get('similarity_score', 0.0)) for c in retrieved_chunks],
                'generated_answer': generation_result.get('answer', '')[:2000],
                'support_label': generation_result.get('support_level'),
                'confidence_score': float(generation_result.get('confidence', 0.0)),
                'latency_ms': int(latency_ms),
                'model_name': getattr(self.config, 'LLM_ENDPOINT', 'unknown'),
                'user_feedback': None,
                'feedback_comment': None,
                'error_message': error[:500] if error else None,
                'created_at': datetime.now()
            }
            
            self.batch_buffer.append(log_entry)
            
            if len(self.batch_buffer) >= self.batch_size:
                self.flush()
                
        except Exception as e:
            logger.error(f"Log entry failed: {e}")
    
    def _classify_query_type(self, classification: Dict) -> str:
        """Classify query type"""
        if classification.get('is_memory_query', False):
            return 'memory'
        elif classification.get('is_domain_relevant', False):
            return 'domain'
        return 'general'
    
    def flush(self):
        """
        FIXED: Explicit flush with retry logic.
        
        Should be called:
        1. When batch is full (automatic)
        2. At end of each user request (explicit in Streamlit)
        3. On program exit (atexit)
        """
        if not self.batch_buffer or self.spark is None:
            return
        
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                df = self.spark.createDataFrame(self.batch_buffer, schema=self.LOG_SCHEMA)
                df.write.format("delta").mode("append").saveAsTable(self.table_name)
                
                logger.info(f"Logged {len(self.batch_buffer)} queries")
                self.batch_buffer.clear()
                return
                
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Flush attempt {attempt + 1} failed: {e}")
                    time.sleep(retry_delay * (2 ** attempt))
                else:
                    logger.error(f"Flush failed after {max_retries} attempts: {e}")
    
    def _flush_on_exit(self):
        """Automatic flush on exit"""
        if self.batch_buffer:
            logger.info("Flushing logs on exit...")
            self.flush()
    
    def get_session_analytics(self, session_id: str) -> Dict:
        """Safe analytics using DataFrame API"""
        if self.spark is None:
            return {}
        
        try:
            logs_df = self.spark.table(self.table_name).filter(
                col("session_id") == session_id
            )
            
            from pyspark.sql.functions import avg, sum as sql_sum, count, when
            
            stats = logs_df.agg(
                count("*").alias("total_queries"),
                avg("latency_ms").alias("avg_latency"),
                avg("confidence_score").alias("avg_confidence"),
                sql_sum(when(col("support_label") == "fully_supported", 1).otherwise(0)).alias("fully_supported"),
                sql_sum(when(col("support_label") == "partially_supported", 1).otherwise(0)).alias("partially_supported"),
                sql_sum(when(col("support_label") == "not_supported", 1).otherwise(0)).alias("not_supported"),
                sql_sum(when(col("error_message").isNotNull(), 1).otherwise(0)).alias("errors")
            ).collect()[0]
            
            return {
                'total_queries': stats['total_queries'],
                'avg_latency_ms': round(stats['avg_latency'], 2) if stats['avg_latency'] else 0,
                'avg_confidence': round(stats['avg_confidence'], 3) if stats['avg_confidence'] else 0,
                'fully_supported': stats['fully_supported'],
                'partially_supported': stats['partially_supported'],
                'not_supported': stats['not_supported'],
                'errors': stats['errors']
            }
            
        except Exception as e:
            logger.error(f"Analytics error: {e}")
            return {}


print("‚úÖ Final Production Query Logger ready")

# COMMAND ----------

# ============================================================
# ALL-IN-ONE: COMPLETE RAG SYSTEM DEPENDENCIES
# Run this cell BEFORE initializing FinalProductionRAG
# ============================================================

import re
import uuid
import time
import logging
import threading
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import deque, defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Memory Query Detector
class MemoryQueryDetector:
    MEMORY_PATTERNS = [
        r'\bfirst\s+question\b', r'\bsecond\s+question\b', r'\bthird\s+question\b',
        r'\blast\s+question\b', r'\bprevious\s+question\b', r'\bearlier\s+question\b',
        r'\b1st\s+question\b', r'\b2nd\s+question\b', r'\b3rd\s+question\b',
        r'\bwhat\s+did\s+i\s+ask\b', r'\bmy\s+questions?\b', r'\bour\s+conversation\b',
        r'\blist\s+(all\s+)?questions?\b', r'\bshow\s+(all\s+)?questions?\b',
        r'\bconversation\s+history\b', r'\bsession\s+summary\b'
    ]
    
    @classmethod
    def is_memory_query(cls, query: str) -> bool:
        return any(re.search(p, query.lower()) for p in cls.MEMORY_PATTERNS)
    
    @classmethod
    def extract_ordinal(cls, query: str) -> Optional[str]:
        ordinals = ['first', 'second', 'third', 'fourth', 'fifth', 
                   '1st', '2nd', '3rd', '4th', '5th', 'last', 'previous', 'latest']
        query_lower = query.lower()
        for ordinal in ordinals:
            if ordinal in query_lower:
                return ordinal
        return None

# Memory Response Generator
class MemoryResponseGenerator:
    @staticmethod
    def handle_memory_query(query: str, memory) -> str:
        if not memory.full_session_log:
            return "You haven't asked any questions yet in this conversation."
        
        ordinal = MemoryQueryDetector.extract_ordinal(query)
        if ordinal:
            turn = memory.recall_by_ordinal(ordinal)
            if turn:
                return f"Your {ordinal} question was:\n\n\"{turn['query']}\"\n\nMy answer was:\n{turn['answer'][:300]}..."
            return f"You haven't asked a {ordinal} question yet."
        
        if any(word in query.lower() for word in ['list', 'all', 'show', 'summary']):
            lines = ["Here are all your questions in this session:\n"]
            for turn in memory.full_session_log:
                lines.append(f"{turn['turn_number']}. {turn['query']}\n   Asked at: {turn['timestamp']}\n")
            return "\n".join(lines)
        
        summary = memory.get_session_summary()
        return (
            f"You've asked {summary['total_turns']} questions in this session.\n"
            f"Domain-specific: {summary['domain_queries']}\n"
            f"General: {summary['general_queries']}\n"
            f"Memory queries: {summary['memory_queries']}"
        )

print("‚úÖ MemoryQueryDetector loaded")
print("‚úÖ MemoryResponseGenerator loaded")
print("\nüéØ Now you can run the FinalProductionRAG cell!")

# COMMAND ----------

# ============================================================
# COMPLETE RAG SYSTEM - ALL COMPONENTS
# ============================================================
%pip install openai
import re
import uuid
import time
import logging
import threading
import os
import atexit
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import deque, defaultdict

from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType,
    DoubleType, TimestampType, BooleanType, ArrayType
)
from databricks.vector_search.client import VectorSearchClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

print("üîß Loading all RAG system components...")
print("=" * 70)

# ============================================================
# 1. SECURE CONFIGURATION
# ============================================================

class SecureConfig:
    """Secure configuration ‚Äî fails fast if required vars are missing"""

    def __init__(self):
        # FIX 4: Require env vars, no hardcoded fallbacks for sensitive values
        self.LLM_ENDPOINT = "databricks-meta-llama-3-3-70b-instruct"
        self.WORKSPACE_URL = os.getenv("DATABRICKS_HOST")
        self.ENDPOINT_NAME = os.getenv("VECTOR_ENDPOINT", "cvip_endpoint")
        self.INDEX_NAME    = os.getenv("VECTOR_INDEX", "workspace.default.cvip_chunks_vs_index")
        
        self.CATALOG = os.getenv("CATALOG", "workspace")
        self.SCHEMA  = os.getenv("SCHEMA",  "default")
        self.LLM_ENDPOINT = os.getenv("LLM_ENDPOINT", "databricks-meta-llama-3-3-70b-instruct")
        self.LLM_TEMPERATURE = 0.1
        self.LLM_MAX_TOKENS  = 800
        
        self.RETRIEVAL_TOP_K  = 15
        self.FINAL_CONTEXT_K  = 7
        
        self.SCORING_WEIGHTS = {
            'foundational':    {'alpha': 0.6, 'beta': 0.4},
            'advanced':        {'alpha': 0.8, 'beta': 0.2},
            'comparison':      {'alpha': 0.7, 'beta': 0.3},
            'implementation':  {'alpha': 0.7, 'beta': 0.3},
            'general':         {'alpha': 0.7, 'beta': 0.3}
        }
        
        self.MIN_SUPPORT_SCORE               = 0.45  # was 0.6
        self.HALLUCINATION_WARNING_THRESHOLD = 0.25  # was 0.4
        self.ENABLE_METADATA_CACHE           = True
        self.CACHE_TTL_SECONDS               = 3600
    
    def validate(self):
        # FIX 4: Hard fail if workspace URL is missing
        if not self.WORKSPACE_URL:
            raise ValueError(
                "DATABRICKS_HOST environment variable is not set. "
                "Set it via os.environ['DATABRICKS_HOST'] = 'https://your-workspace.azuredatabricks.net'"
            )
        required = ['ENDPOINT_NAME', 'INDEX_NAME']
        for attr in required:
            if not getattr(self, attr):
                raise ValueError(f"Missing required configuration: {attr}")
        logger.info("‚úÖ Configuration validated")

print("‚úÖ SecureConfig")

# ============================================================
# 2. METADATA CACHE  (FIX 7: lock all reads too)
# ============================================================

class MetadataCache:
    """Thread-safe in-memory cache for tier metadata"""
    
    def __init__(self, ttl_seconds: int = 3600):
        self._cache: Dict[str, Dict] = {}
        self._cache_time: Optional[datetime] = None
        self._lock = threading.Lock()          # protects ALL access
        self.ttl_seconds = ttl_seconds
    
    def get(self, chunk_id: str) -> Optional[Dict]:
        # FIX 7: read under lock
        with self._lock:
            if self._is_expired_unlocked():
                return None
            return self._cache.get(chunk_id)
    
    def bulk_get(self, chunk_ids: List[str]) -> Dict[str, Dict]:
        # FIX 7: read under lock
        with self._lock:
            if self._is_expired_unlocked():
                return {}
            return {cid: self._cache[cid] for cid in chunk_ids if cid in self._cache}
    
    def update(self, metadata_dict: Dict[str, Dict]):
        with self._lock:
            self._cache.update(metadata_dict)
            self._cache_time = datetime.now()
    
    def _is_expired_unlocked(self) -> bool:
        """Call only when lock is already held."""
        if self._cache_time is None:
            return True
        return (datetime.now() - self._cache_time).total_seconds() > self.ttl_seconds
    
    def get_stats(self) -> Dict:
        with self._lock:
            return {
                'cache_size':  len(self._cache),
                'is_expired':  self._is_expired_unlocked(),
                'last_update': self._cache_time.isoformat() if self._cache_time else None
            }
    
    def clear(self):
        with self._lock:
            self._cache.clear()
            self._cache_time = None

class EnhancedMetadataCache(MetadataCache):
    """Enhanced cache ‚Äî identical to MetadataCache, kept for compatibility."""
    pass

print("‚úÖ MetadataCache")

# ============================================================
# 3. SAFE METADATA FETCHER  (FIX 3 + FIX 8)
# ============================================================

class SafeMetadataFetcher:
    """Metadata fetcher with full preload ‚Äî zero Spark calls during queries"""

    def __init__(self, cache: MetadataCache, catalog: str = "workspace", schema: str = "default"):
        self.cache = cache
        self.catalog = catalog
        self.schema = schema
        self.spark = SparkSession.getActiveSession()
        if self.spark is None:
            raise RuntimeError("No active Spark session found")
        self._preloaded: Dict[str, Dict] = {}
        self._preload_all_metadata()

    def _preload_all_metadata(self):
        logger.info("üì• Preloading metadata into memory...")
        try:
            rows = (
                self.spark.table(f"{self.catalog}.{self.schema}.cvip_chunks")
                .join(
                    self.spark.table(f"{self.catalog}.{self.schema}.cvip_documents"),
                    on="document_id",
                    how="left"
                )
                .select("chunk_id", "source_tier", "priority_score", "source_type", "source_name")
                .collect()
            )
            self._preloaded = {
                row["chunk_id"]: {
                    "source_tier":    row["source_tier"],
                    "priority_score": row["priority_score"],
                    "source_type":    row["source_type"],
                    "source_name":    row["source_name"],
                }
                for row in rows
            }
            logger.info(f"‚úÖ Preloaded metadata for {len(self._preloaded)} chunks")
        except Exception as e:
            logger.error(f"‚ùå Metadata preload failed: {e}")
            self._preloaded = {}

    def fetch_metadata(self, chunk_ids: List[str]) -> Dict[str, Dict]:
        if not chunk_ids:
            return {}
        return {cid: self._preloaded[cid] for cid in chunk_ids if cid in self._preloaded}

print("‚úÖ SafeMetadataFetcher")
# ============================================================
# 4. SAFE RESULT PARSER
# ============================================================

class SafeResultParser:
    """Robust result parsing with explicit schema handling"""
    
    @staticmethod
    def parse_vector_search_results(results: Dict, expected_columns: List[str]) -> List[Dict]:
        chunks = []
        if not results or 'result' not in results:
            return chunks
        
        result_data  = results['result']
        column_names = result_data.get('columns', expected_columns)
        data_array   = result_data.get('data_array', [])
        column_map   = {name: idx for idx, name in enumerate(column_names)}
        
        for row in data_array:
            try:
                chunk = {}
                for col_name in expected_columns:
                    idx = column_map.get(col_name)
                    chunk[col_name] = row[idx] if (idx is not None and idx < len(row)) else None
                
                if 'score' in column_map:
                    chunk['similarity_score'] = row[column_map['score']]
                elif len(row) > len(expected_columns):
                    chunk['similarity_score'] = row[-1]
                else:
                    chunk['similarity_score'] = 0.0
                
                chunks.append(chunk)
            except Exception as e:
                logger.warning(f"Error parsing result row: {e}")
        
        return chunks

print("‚úÖ SafeResultParser")

# ============================================================
# 5. QUERY CLASSIFIER
# ============================================================

class QueryClassifier:
    """Classify queries by intent and domain relevance"""
    
    DOMAIN_KEYWORDS = {
    'core':         ['image', 'pixel', 'computer vision', 'visual', 'picture', 
                     'digital image', 'intensity', 'gray level', 'spatial'],
    'algorithms':   ['edge detection', 'sobel', 'canny', 'convolution', 'filter',
                     'segmentation', 'thresholding', 'histogram', 'gradient'],
    'transforms':   ['fourier', 'fft', 'dft', 'frequency domain', 'wavelet',
                     'laplacian', 'transform', 'frequency'],
    'features':     ['sift', 'surf', 'orb', 'feature detection', 'keypoint',
                     'descriptor', 'harris', 'corner detection'],
    'deep_learning':['cnn', 'neural', 'resnet', 'vgg', 'yolo', 'transformer',
                     'vit', 'deep learning', 'convolutional', 'activation'],
    'operations':   ['morphology', 'erosion', 'dilation', 'enhancement',
                     'point operation', 'linear', 'nonlinear', 'spatial filtering'],
    '3d_vision':    ['camera calibration', 'intrinsic', 'extrinsic', '3d reconstruction',
                     'point cloud', 'stereo', 'depth', 'epipolar', 'homography',
                     'perspective', 'projection'],
    'geometry':     ['affine', 'geometric transformation', 'rotation', 'translation',
                     'scaling', 'interpolation', 'resampling'],
    'restoration':  ['noise', 'denoising', 'restoration', 'blur', 'deblur',
                     'wiener', 'degradation'],
    'compression':  ['compression', 'jpeg', 'encoding', 'huffman', 'quantization'],
    'color':        ['color', 'rgb', 'hsv', 'hsi', 'chrominance', 'luminance',
                     'color space', 'saturation']
}
    
    INTENT_PATTERNS = {
        'foundational':    [r'\bwhat is\b', r'\bdefine\b', r'\bexplain\b', r'\bbasics?\b', r'\bfundamental'],
        'advanced':        [r'\bstate[- ]of[- ]the[- ]art\b', r'\bSOTA\b', r'\blatest\b', r'\bcutting[- ]edge\b'],
        'comparison':      [r'\bcompare\b', r'\bvs\.?\b', r'\bdifference between\b', r'\bbetter\b'],
        'implementation':  [r'\bhow to implement\b', r'\bcode\b', r'\bpython\b', r'\bopencv\b']
    }
    
    @classmethod
    def classify(cls, query: str) -> Dict:
        query_lower = query.lower()
        
        matched_keywords = [
            (cat, kw)
            for cat, kws in cls.DOMAIN_KEYWORDS.items()
            for kw in kws
            if kw in query_lower
        ]
        
        is_domain_relevant = len(matched_keywords) > 0
        
        intent_scores = {
            intent_type: sum(1 for p in patterns if re.search(p, query_lower))
            for intent_type, patterns in cls.INTENT_PATTERNS.items()
        }
        intent_scores = {k: v for k, v in intent_scores.items() if v > 0}
        intent = max(intent_scores, key=intent_scores.get) if intent_scores else 'general'
        
        confidence = min(len(matched_keywords) / 3.0, 1.0) if is_domain_relevant else 0.0
        
        return {
            'is_domain_relevant': is_domain_relevant,
            'intent':             intent,
            'confidence':         confidence,
            'matched_keywords':   [kw for _, kw in matched_keywords[:5]]
        }

print("‚úÖ QueryClassifier")

# ============================================================
# 6. MEMORY COMPONENTS
# ============================================================

class MemoryQueryDetector:
    MEMORY_PATTERNS = [
        # Ordinal + question patterns
        r'\bfirst\s+question\b',    r'\bsecond\s+question\b',
        r'\bthird\s+question\b',    r'\bfourth\s+question\b',
        r'\bfifth\s+question\b',    r'\bsixth\s+question\b',
        r'\bseventh\s+question\b',  r'\beighth\s+question\b',
        r'\bninth\s+question\b',    r'\btenth\s+question\b',
        r'\blast\s+question\b',     r'\bprevious\s+question\b',
        r'\bearlier\s+question\b',
        r'\b1st\s+question\b',      r'\b2nd\s+question\b',
        r'\b3rd\s+question\b',      r'\b4th\s+question\b',
        r'\b5th\s+question\b',      r'\b6th\s+question\b',
        r'\b7th\s+question\b',      r'\b8th\s+question\b',
        r'\b9th\s+question\b',      r'\b10th\s+question\b',
        # General memory patterns
        r'\bwhat\s+did\s+i\s+ask\b',
        r'\bmy\s+questions?\b',
        r'\bour\s+conversation\b',
        r'\blist\s+(all\s+)?questions?\b',
        r'\bshow\s+(all\s+)?questions?\b',
        r'\ball\s+questions?\b',
        r'\bquestions?\s+i\s+asked\b',
        r'\bwhat\s+i\s+asked\b',
        r'\bwhat\s+was\s+my\b',
        r'\bwhat\s+were\s+my\b',
        r'\brecall\b',
        r'\bremember\s+my\b',
        r'\bconversation\s+history\b',
        r'\basked\s+before\b',
    ]
    
    @classmethod
    def is_memory_query(cls, query: str) -> bool:
        return any(re.search(p, query.lower()) for p in cls.MEMORY_PATTERNS)
    @classmethod
    def extract_ordinal(cls, query: str) -> Optional[str]:
        ordinals = [
            'first', 'second', 'third', 'fourth', 'fifth',
            'sixth', 'seventh', 'eighth', 'ninth', 'tenth',
            '1st', '2nd', '3rd', '4th', '5th',
            '6th', '7th', '8th', '9th', '10th',
            'last', 'previous', 'latest'
            ]
        query_lower = query.lower()
        return next((o for o in ordinals if o in query_lower), None)

print("‚úÖ MemoryQueryDetector")
class MemoryResponseGenerator:
    @staticmethod
    def handle_memory_query(query: str, memory) -> str:
        if not memory.full_session_log:
            return "You haven't asked any questions yet in this conversation."

        ordinal = MemoryQueryDetector.extract_ordinal(query)
        if ordinal:
            turn = memory.recall_by_ordinal(ordinal)
            if turn:
                return (
                    f"Your {ordinal} question was:\n\n\"{turn['query']}\"\n\n"
                    f"My answer was:\n{turn['answer'][:300]}..."
                )
            return f"You haven't asked a {ordinal} question yet."

        if any(w in query.lower() for w in ['list', 'all', 'show', 'summary']):
            lines = ["Here are all your questions in this session:\n"]
            for turn in memory.full_session_log:
                lines.append(f"{turn['turn_number']}. {turn['query']}")
            return "\n".join(lines)

        s = memory.get_session_summary()
        return (
            f"You've asked {s['total_turns']} questions in this session.\n"
            f"Domain-specific: {s['domain_queries']}\n"
            f"General: {s['general_queries']}"
        )


print("‚úÖ MemoryResponseGenerator")   

# ============================================================
# 7. PRODUCTION MEMORY SYSTEM  (FIX 1: tiktoken logic fixed)
# ============================================================

class _OfflineTokenizer:
    """Drop-in replacement for tiktoken in air-gapped environments."""
    def encode(self, text: str) -> list:
        return list(range(max(1, len(text) // 4)))
    def decode(self, tokens: list) -> str:
        raise NotImplementedError("Decode not supported in offline tokenizer")

# FIX 1: actually attempt the import, use real tokenizer if available
try:
    import tiktoken
    _tokenizer_instance = tiktoken.get_encoding("cl100k_base")
    HAS_TIKTOKEN = True
    logger.info("‚úÖ tiktoken loaded")
except Exception:
    _tokenizer_instance = _OfflineTokenizer()
    HAS_TIKTOKEN = False
    logger.warning("‚ö†Ô∏è tiktoken unavailable ‚Äî using offline tokenizer")


class ProductionMemorySystem:
    
    def __init__(self, max_window_size: int = 10, max_context_tokens: int = 2000,
                 enable_persistence: bool = False):
        self.max_window_size   = max_window_size
        self.max_context_tokens = max_context_tokens
        self.enable_persistence = enable_persistence
        
        self.sliding_window    = deque(maxlen=max_window_size)
        self.full_session_log: List[Dict] = []
        self.conversation_summary: Optional[str] = None
        
        self.session_id    = str(uuid.uuid4())
        self.session_start = datetime.now()
        self.turn_count    = 0
        
        # FIX 1: use whichever tokenizer was resolved at module load
        self.tokenizer = _tokenizer_instance
        self.recency_decay = 0.9
    
    def add_turn(self, query: str, answer: str, metadata: Dict) -> Dict:
        self.turn_count += 1
        turn_entry = {
            'turn_id':       str(uuid.uuid4()),
            'turn_number':   self.turn_count,
            'session_id':    self.session_id,
            'timestamp':     datetime.now().isoformat(),
            'query':         query,
            'answer':        answer,
            'metadata':      metadata,
            'base_salience': self._calculate_base_salience(metadata)
        }
        self.sliding_window.append(turn_entry)
        self.full_session_log.append(turn_entry)
        return turn_entry
    
    def _calculate_base_salience(self, metadata: Dict) -> float:
        confidence    = metadata.get('confidence', 0.5)
        support_level = metadata.get('support_level', 'not_supported')
        support_boost = {
            'fully_supported':    1.0,
            'partially_supported': 0.7,
            'not_supported':      0.3,
            'memory_recall':      0.5,
            'out_of_domain':      0.2,
            'error':              0.1
        }.get(support_level, 0.5)
        return 0.5 * confidence + 0.5 * support_boost
    
    def recall_by_ordinal(self, ordinal: str) -> Optional[Dict]:
        ordinal_map = {
           'first':   0, '1st': 0,
           'second':  1, '2nd': 1,
           'third':   2, '3rd': 2,
           'fourth':  3, '4th': 3,
           'fifth':   4, '5th': 4,
           'sixth':   5, '6th': 5,
           'seventh': 6, '7th': 6,
           'eighth':  7, '8th': 7,
           'ninth':   8, '9th': 8,
           'tenth':   9, '10th': 9,
           'last': -1, 'latest': -1, 'previous': -1
       }
        index = ordinal_map.get(ordinal.lower())
        if index is None:
           return None
        try:
           return self.full_session_log[index]
        except IndexError:
           return None
    
    def get_session_summary(self) -> Dict:
        duration = datetime.now() - self.session_start
        avg_salience = (
            sum(t['base_salience'] for t in self.full_session_log) / len(self.full_session_log)
            if self.full_session_log else 0.0
        )
        domain_queries = sum(
            1 for t in self.full_session_log
            if t['metadata'].get('query_classification', {}).get('is_domain_relevant', False)
        )
        memory_queries = sum(
            1 for t in self.full_session_log
            if t['metadata'].get('query_classification', {}).get('is_memory_query', False)
        )
        return {
            'session_id':       self.session_id,
            'session_start':    self.session_start.isoformat(),
            'session_duration': str(duration).split('.')[0],
            'total_turns':      self.turn_count,
            'domain_queries':   domain_queries,
            'memory_queries':   memory_queries,
            'general_queries':  self.turn_count - domain_queries - memory_queries,
            'avg_salience':     round(avg_salience, 3),
            'has_summary':      self.conversation_summary is not None
        }

print("‚úÖ ProductionMemorySystem")

# ============================================================
# 8. GROUNDING CHECKER
# ============================================================

class ImprovedGroundingChecker:
    
    @staticmethod
    def verify_grounding(answer: str, retrieved_chunks: List[Dict], query: str, config) -> Dict:
        overlap_score   = ImprovedGroundingChecker._calculate_overlap(answer, retrieved_chunks)
        citation_score  = ImprovedGroundingChecker._check_citations(answer, retrieved_chunks)
        alignment_score = ImprovedGroundingChecker._check_sentence_alignment(answer, retrieved_chunks)
        
        final_score = 0.4 * overlap_score + 0.3 * citation_score + 0.3 * alignment_score
        
        if final_score >= config.MIN_SUPPORT_SCORE:
            support_level = 'fully_supported'
        elif final_score >= config.HALLUCINATION_WARNING_THRESHOLD:
            support_level = 'partially_supported'
        else:
            support_level = 'not_supported'
        
        citations      = re.findall(r'\[Source:([^\]]+)\]', answer)
        used_chunk_ids = [
            chunk['chunk_id'] for chunk in retrieved_chunks[:5]
            if chunk.get('citation_label', '') in answer or chunk.get('source_name', '') in answer
        ]
        
        return {
            'citations':      citations,
            'support_level':  support_level,
            'confidence':     final_score,
            'used_chunk_ids': used_chunk_ids,
            'scores': {'overlap': overlap_score, 'citation': citation_score, 'alignment': alignment_score}
        }
    
    @staticmethod
    def _calculate_overlap(answer: str, chunks: List[Dict]) -> float:
        answer_terms  = set(re.findall(r'\b\w{4,}\b', answer.lower()))
        context_terms = set(
            term for chunk in chunks
            for term in re.findall(r'\b\w{4,}\b', chunk['content'].lower())
        )
        if not answer_terms:
            return 0.0
        return len(answer_terms & context_terms) / len(answer_terms)
    
    @staticmethod
    def _check_citations(answer: str, chunks: List[Dict]) -> float:
        citations  = re.findall(r'\[Source:', answer)
        word_count = len(answer.split())
        if not citations:
            return 0.0
        expected = max(1, word_count // 100)
        return min(len(citations) / expected, 1.0)
    
    @staticmethod
    def _check_sentence_alignment(answer: str, chunks: List[Dict]) -> float:
        sentences = [s.strip() for s in re.split(r'[.!?]+', answer) if len(s.strip()) > 10]
        if not sentences:
            return 0.0
        context_lower = " ".join(c['content'] for c in chunks).lower()
        aligned = sum(
            1 for s in sentences
            if any(phrase in context_lower for phrase in re.findall(r'\b\w{5,}\b', s.lower()))
        )
        return aligned / len(sentences)

print("‚úÖ ImprovedGroundingChecker")

# ============================================================
# 9. ANSWER GENERATOR  (FIX 6: explicit mock flag)
# ============================================================
class FixedAnswerGenerator:
    MOCK_MODE = False
    
    def __init__(self, config):
        self.config = config
        if self.MOCK_MODE:
            logger.warning("‚ö†Ô∏è  Running in MOCK_MODE ‚Äî responses are stub text only.")
        else:
            logger.info(f"‚úÖ Real LLM initialized: {config.LLM_ENDPOINT}")
    
    def generate(self, query: str, retrieved_chunks: List[Dict], query_classification: Dict) -> Dict:
        if not retrieved_chunks:
            return {
                'answer': "I couldn't find relevant information in my knowledge base.",
                'citations': [], 'support_level': 'not_supported',
                'confidence': 0.0, 'used_chunks': [], 'grounding_scores': {}, 'error': None
            }
        
        if self.MOCK_MODE:
            chunk = retrieved_chunks[0]
            answer = (
                f"[MOCK] Based on retrieved information: "
                f"{chunk['content'][:200]}... [Source: {chunk['citation_label']}]"
            )
        else:
            answer = self._call_llm(query, retrieved_chunks, query_classification)
        
        grounding = ImprovedGroundingChecker.verify_grounding(
            answer, retrieved_chunks, query, self.config
        )
        
        return {
            'answer':           answer,
            'citations':        grounding['citations'],
            'support_level':    grounding['support_level'],
            'confidence':       grounding['confidence'],
            'used_chunks':      grounding['used_chunk_ids'],
            'grounding_scores': grounding['scores'],
            'error':            None
        }

    def _call_llm(self, query: str, chunks: List[Dict], classification: Dict) -> str:
        import requests
        
        context = "\n\n".join([
            f"[Source: {c.get('citation_label', 'Unknown')}]\n{c['content'][:400]}"
            for c in chunks[:5]
        ])
        
        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        
        url = f"{self.config.WORKSPACE_URL}/serving-endpoints/{self.config.LLM_ENDPOINT}/invocations"
        
        r = requests.post(
            url,
            headers={
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            },
            json={
                "messages": [
                    {
                        "role": "system",
"content": (
    "You are an expert professor in Computer Vision and Image Processing. "
    "Your answers should be comprehensive, well-structured, and educational. "
    "Follow these guidelines:\n"
    "1. Give detailed explanations with proper definitions\n"
    "2. Always include a worked example when explaining algorithms or operations\n"
    "3. Use mathematical notation where appropriate\n"
    "4. Structure your answer with clear sections using markdown headers\n"
    "5. Cite sources using [Source: name, p.X] format after each key point\n"
    "6. If the context has insufficient detail, explain what you know from the context "
    "and indicate where more detail would be needed\n"
    "7. Aim for thorough, textbook-quality answers"
)
                    },
                    {
                        "role": "user",
                        "content": f"Context:\n{context}\n\nQuestion: {query}"
                    }
                ],
                "max_tokens": self.config.LLM_MAX_TOKENS,
                "temperature": self.config.LLM_TEMPERATURE,
                "stream": False
            },
            timeout=120
        )
        
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"]


print("‚úÖ FixedAnswerGenerator")
# ============================================================
# 10. QUERY LOGGER  (FIX 5: memory query type added)
# ============================================================

class FinalQueryLogger:
    
    LOG_SCHEMA = StructType([
        StructField("query_id",           StringType(),         False),
        StructField("session_id",         StringType(),         False),
        StructField("user_query",         StringType(),         False),
        StructField("query_type",         StringType(),         True),   # domain|memory|general
        StructField("routed_to_retrieval",BooleanType(),        True),
        StructField("top_k",              IntegerType(),        True),
        StructField("retrieved_chunk_ids",ArrayType(StringType()),True),
        StructField("retrieved_scores",   ArrayType(DoubleType()),True),
        StructField("generated_answer",   StringType(),         True),
        StructField("support_label",      StringType(),         True),
        StructField("confidence_score",   DoubleType(),         True),
        StructField("latency_ms",         IntegerType(),        True),
        StructField("model_name",         StringType(),         True),
        StructField("user_feedback",      StringType(),         True),
        StructField("feedback_comment",   StringType(),         True),
        StructField("error_message",      StringType(),         True),
        StructField("created_at",         TimestampType(),      False)
    ])
    
    def __init__(self, config):
        self.config       = config
        self.batch_buffer: List[Dict] = []
        self.batch_size   = 10
        catalog = getattr(config, 'CATALOG', 'workspace')
        schema  = getattr(config, 'SCHEMA',  'default')
        self.table_name   = f"{catalog}.{schema}.cvip_query_logs"
        self.spark        = SparkSession.getActiveSession()
        if self.spark:
            atexit.register(self._flush_on_exit)
    
    @staticmethod
    def _resolve_query_type(classification: Dict) -> str:
        # FIX 5: three-way classification
        if classification.get('is_memory_query'):
            return 'memory'
        if classification.get('is_domain_relevant'):
            return 'domain'
        return 'general'
    
    def log_query(self, session_id: str, query: str, query_classification: Dict,
                  retrieved_chunks: List[Dict], generation_result: Dict,
                  latency_ms: int, error: Optional[str] = None):
        if self.spark is None:
            return
        try:
            self.batch_buffer.append({
                'query_id':            str(uuid.uuid4()),
                'session_id':          session_id,
                'user_query':          query[:1000],
                'query_type':          self._resolve_query_type(query_classification),
                'routed_to_retrieval': query_classification.get('is_domain_relevant', False),
                'top_k':               len(retrieved_chunks),
                'retrieved_chunk_ids': [c.get('chunk_id', '') for c in retrieved_chunks],
                'retrieved_scores':    [float(c.get('similarity_score', 0.0)) for c in retrieved_chunks],
                'generated_answer':    generation_result.get('answer', '')[:2000],
                'support_label':       generation_result.get('support_level'),
                'confidence_score':    float(generation_result.get('confidence', 0.0)),
                'latency_ms':          int(latency_ms),
                'model_name':          getattr(self.config, 'LLM_ENDPOINT', 'unknown'),
                'user_feedback':       None,
                'feedback_comment':    None,
                'error_message':       error[:500] if error else None,
                'created_at':          datetime.now()
            })
            if len(self.batch_buffer) >= self.batch_size:
                self.flush()
        except Exception as e:
            logger.error(f"Logging error: {e}")
    
    def flush(self):
        if not self.batch_buffer or self.spark is None:
            return
        for attempt in range(3):
            try:
                df = self.spark.createDataFrame(self.batch_buffer, schema=self.LOG_SCHEMA)
                df.write.format("delta").mode("append").saveAsTable(self.table_name)
                logger.info(f"Logged {len(self.batch_buffer)} queries to {self.table_name}")
                self.batch_buffer.clear()
                return
            except Exception as e:
                if attempt < 2:
                    time.sleep(2 ** attempt)
                else:
                    logger.error(f"Flush failed after 3 attempts: {e}")
    
    def _flush_on_exit(self):
        if self.batch_buffer:
            self.flush()
    
    def get_session_analytics(self, session_id: str) -> Dict:
        if self.spark is None:
            return {}
        try:
            from pyspark.sql.functions import avg, sum as sql_sum, count, when
            stats = (
                self.spark.table(self.table_name)
                .filter(col("session_id") == session_id)
                .agg(
                    count("*").alias("total_queries"),
                    avg("latency_ms").alias("avg_latency"),
                    avg("confidence_score").alias("avg_confidence"),
                    sql_sum(when(col("support_label") == "fully_supported",    1).otherwise(0)).alias("fully_supported"),
                    sql_sum(when(col("support_label") == "partially_supported",1).otherwise(0)).alias("partially_supported"),
                    sql_sum(when(col("support_label") == "not_supported",      1).otherwise(0)).alias("not_supported"),
                    sql_sum(when(col("error_message").isNotNull(),             1).otherwise(0)).alias("errors")
                ).collect()[0]
            )
            return {
                'total_queries':      stats['total_queries'],
                'avg_latency_ms':     round(stats['avg_latency'],    2) if stats['avg_latency']    else 0,
                'avg_confidence':     round(stats['avg_confidence'], 3) if stats['avg_confidence'] else 0,
                'fully_supported':    stats['fully_supported'],
                'partially_supported':stats['partially_supported'],
                'not_supported':      stats['not_supported'],
                'errors':             stats['errors']
            }
        except Exception as e:
            logger.error(f"Analytics error: {e}")
            return {}

print("‚úÖ FinalQueryLogger")

# ============================================================
# 11. CROSS-ENCODER MANAGER
# ============================================================

class CrossEncoderManager:
    def __init__(self, enabled: bool = False):
        self.enabled = enabled
        self.model   = None
        if enabled:
            logger.info("‚ö†Ô∏è Cross-encoder stub ‚Äî reranking not yet implemented")
    
    def rerank(self, query: str, chunks: List[Dict]) -> List[Dict]:
        for chunk in chunks:
            chunk['final_score'] = chunk.get('weighted_score', 0.0)
        return chunks

print("‚úÖ CrossEncoderManager")

# ============================================================
# 12. SMART FLUSH MANAGER  (FIX 2: RLock prevents deadlock)
# ============================================================

class SmartFlushManager:
    
    def __init__(self, logger, flush_every_n: int = 5, flush_every_seconds: int = 30):
        self.logger             = logger
        self.flush_every_n      = flush_every_n
        self.flush_every_seconds = flush_every_seconds
        self.queries_since_flush = 0
        self.last_flush_time    = time.time()
        # FIX 2: RLock allows the same thread to re-acquire without deadlock
        self._lock = threading.RLock()
    
    def _needs_flush(self) -> bool:
        """Internal check ‚Äî assumes lock is held."""
        if self.queries_since_flush >= self.flush_every_n:
            return True
        return (time.time() - self.last_flush_time) >= self.flush_every_seconds
    
    def increment_and_flush_if_needed(self):
        with self._lock:
            self.queries_since_flush += 1
            if self._needs_flush():          # safe: RLock is re-entrant
                self._do_flush()
    
    def _do_flush(self):
        try:
            self.logger.flush()
            self.queries_since_flush = 0
            self.last_flush_time     = time.time()
        except Exception as e:
            logger.error(f"Flush error: {e}")
    
    def force_flush(self):
        with self._lock:
            self._do_flush()

print("‚úÖ SmartFlushManager")

# ============================================================
# 13. SESSION COMPONENTS
# ============================================================

class SessionComponents:
    
    def __init__(self, enable_persistence: bool = False):
        self.memory = ProductionMemorySystem(
            max_window_size=10,
            max_context_tokens=2000,
            enable_persistence=enable_persistence
        )
        self.health_stats = {
            'total_queries': 0, 'successful_queries': 0,
            'failed_queries': 0, 'avg_latency': 0.0,
            'session_start': datetime.now()
        }
        self._lock = threading.Lock()
    
    def update_health_stats(self, success: bool, latency_ms: int):
        with self._lock:
            self.health_stats['total_queries'] += 1
            if success:
                self.health_stats['successful_queries'] += 1
            else:
                self.health_stats['failed_queries'] += 1
            total = self.health_stats['total_queries']
            self.health_stats['avg_latency'] = (
                (self.health_stats['avg_latency'] * (total - 1) + latency_ms) / total
            )
    
    def get_health_stats(self) -> Dict:
        with self._lock:
            return self.health_stats.copy()

print("‚úÖ SessionComponents")

# ============================================================
# COMPLETION
# ============================================================
print("=" * 70)
print("‚úÖ ALL COMPONENTS LOADED ‚Äî all 8 issues resolved")
print("=" * 70)
print(f"   Tokenizer : {'tiktoken (cl100k_base)' if HAS_TIKTOKEN else 'OfflineTokenizer'}")
print(f"   Generator : {'MOCK MODE' if FixedAnswerGenerator.MOCK_MODE else 'LIVE'}")
print("üöÄ Ready to initialize FinalProductionRAG!")

# COMMAND ----------

# Now run the FinalProductionRAG cell
# It should work now!

print("üîç Verifying all dependencies are loaded...")

required = [
    'SecureConfig', 'MetadataCache', 'SafeMetadataFetcher', 
    'CrossEncoderManager', 'FixedAnswerGenerator', 'ProductionMemorySystem',
    'FinalQueryLogger', 'SafeResultParser', 'QueryClassifier',
    'MemoryQueryDetector', 'MemoryResponseGenerator', 'ImprovedGroundingChecker',
    'SmartFlushManager', 'SessionComponents', 'EnhancedMetadataCache'
]

missing = [c for c in required if c not in globals()]

if missing:
    print(f"‚ùå Still missing: {', '.join(missing)}")
else:
    print("‚úÖ All dependencies verified!")
    print("\nüéØ Now you can run the FinalProductionRAG initialization cell")

# COMMAND ----------

# ============================================================
# FINAL PRODUCTION RAG CONTROLLER
# ============================================================

import time
import logging
import threading
from typing import Dict, List, Optional
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_dependencies():
    required = [
        'SecureConfig', 'MetadataCache', 'EnhancedMetadataCache',
        'SafeMetadataFetcher', 'CrossEncoderManager', 'FixedAnswerGenerator',
        'ProductionMemorySystem', 'FinalQueryLogger', 'SafeResultParser',
        'QueryClassifier', 'MemoryQueryDetector', 'MemoryResponseGenerator',
        'ImprovedGroundingChecker', 'SmartFlushManager', 'SessionComponents'
    ]
    missing = [c for c in required if c not in globals()]
    if missing:
        raise RuntimeError(f"Missing components: {', '.join(missing)}\nRun the components cell first.")
    logger.info("‚úÖ All dependencies verified")

check_dependencies()


class FinalProductionRAG:
    
    def __init__(
        self,
        enable_reranking: bool = False,
        enable_persistence: bool = False,
        flush_every_n: int = 5,
        flush_every_seconds: int = 30
    ):
        logger.info("=" * 70)
        logger.info("üöÄ Initializing Final Production RAG System")
        logger.info("=" * 70)
        
        self.enable_reranking   = enable_reranking
        self.enable_persistence = enable_persistence
        
        # General knowledge answer cache ‚Äî avoids repeated LLM calls
        self._general_cache: Dict[str, str] = {}
        
        self._initialize_shared_components()
        
        self.sessions: Dict[str, SessionComponents] = {}
        self.sessions_lock = threading.Lock()
        
        self.flush_manager = SmartFlushManager(
            self.logger,
            flush_every_n=flush_every_n,
            flush_every_seconds=flush_every_seconds
        )
        
        self.system_stats = {
            'system_start':    datetime.now(),
            'total_sessions':  0,
            'active_sessions': 0
        }
        
        logger.info("=" * 70)
        logger.info("‚úÖ System Ready!")
        logger.info(f"   Reranking:   {'Enabled' if enable_reranking else 'Disabled'}")
        logger.info(f"   Persistence: {'Enabled' if enable_persistence else 'Disabled'}")
        logger.info(f"   Flush Strategy: Every {flush_every_n} queries OR {flush_every_seconds}s")
        logger.info("=" * 70)
    
    def _initialize_shared_components(self):
        try:
            logger.info("üìã Loading configuration...")
            self.config = SecureConfig()
            self.config.validate()
            
            logger.info("üíæ Initializing metadata cache...")
            self.metadata_cache = EnhancedMetadataCache(
                ttl_seconds=self.config.CACHE_TTL_SECONDS
            )
            
            logger.info("üîç Initializing metadata fetcher...")
            self.metadata_fetcher = SafeMetadataFetcher(
                self.metadata_cache,
                catalog=self.config.CATALOG,
                schema=self.config.SCHEMA
            )
            
            logger.info("üîÑ Initializing cross-encoder...")
            self.cross_encoder = CrossEncoderManager(enabled=self.enable_reranking)
            
            logger.info("üìö Initializing retriever...")
            self.retriever = self._build_retriever()
            
            logger.info("ü§ñ Initializing LLM...")
            self.generator = FixedAnswerGenerator(self.config)
            
            logger.info("üìä Initializing logger...")
            self.logger = FinalQueryLogger(self.config)
            
        except Exception as e:
            logger.error(f"‚ùå Initialization failed: {e}")
            raise RuntimeError(f"Failed to initialize: {e}")
    
    def _build_retriever(self):
        from databricks.vector_search.client import VectorSearchClient
        
        vsc = VectorSearchClient(
            workspace_url=self.config.WORKSPACE_URL,
            disable_notice=True
        )
        index = vsc.get_index(
            endpoint_name=self.config.ENDPOINT_NAME,
            index_name=self.config.INDEX_NAME
        )
        
        class OptimizedRetriever:
            def __init__(self, index, metadata_fetcher, cross_encoder, config):
                self.index            = index
                self.metadata_fetcher = metadata_fetcher
                self.cross_encoder    = cross_encoder
                self.config           = config
            
            def retrieve(self, query, query_classification, top_k=None, use_reranking=False):
                if top_k is None:
                    top_k = self.config.FINAL_CONTEXT_K
                
                intent  = query_classification.get('intent', 'general')
                weights = self.config.SCORING_WEIGHTS.get(intent, self.config.SCORING_WEIGHTS['general'])
                alpha, beta = weights['alpha'], weights['beta']
                
                retrieval_k = self.config.RETRIEVAL_TOP_K if use_reranking else top_k * 2
                
                try:
                    results = self.index.similarity_search(
                        query_text=query,
                        columns=["chunk_id", "content", "citation_label", "page_number", "document_id"],
                        num_results=retrieval_k
                    )
                except Exception as e:
                    logger.error(f"Vector search failed: {e}")
                    return []
                
                chunks = SafeResultParser.parse_vector_search_results(
                    results,
                    expected_columns=["chunk_id", "content", "citation_label", "page_number", "document_id"]
                )
                
                if not chunks:
                    return []
                
                chunk_ids     = [c['chunk_id'] for c in chunks]
                tier_metadata = self.metadata_fetcher.fetch_metadata(chunk_ids)
                
                for chunk in chunks:
                    meta = tier_metadata.get(chunk['chunk_id'], {})
                    chunk['source_tier']    = meta.get('source_tier', 3)
                    chunk['priority_score'] = meta.get('priority_score', 0.5)
                    chunk['source_type']    = meta.get('source_type', 'unknown')
                    chunk['source_name']    = meta.get('source_name', 'Unknown')
                    chunk['weighted_score'] = (
                        alpha * chunk.get('similarity_score', 0.0) +
                        beta  * chunk.get('priority_score', 0.5)
                    )
                    chunk['alpha'] = alpha
                    chunk['beta']  = beta
                
                chunks.sort(key=lambda x: x['weighted_score'], reverse=True)
                
                if use_reranking and len(chunks) > top_k:
                    chunks = self.cross_encoder.rerank(query, chunks[:top_k * 2])
                else:
                    for chunk in chunks:
                        chunk['final_score'] = chunk['weighted_score']
                
                return chunks[:top_k]
        
        return OptimizedRetriever(index, self.metadata_fetcher, self.cross_encoder, self.config)
    
    def _get_or_create_session(self, session_id: str = None) -> SessionComponents:
        if session_id is None:
            session_id = "default"
        
        with self.sessions_lock:
            if session_id not in self.sessions:
                self.sessions[session_id] = SessionComponents(self.enable_persistence)
                self.system_stats['total_sessions'] += 1
                logger.info(f"Created new session: {session_id}")
            self.system_stats['active_sessions'] = len(self.sessions)
            return self.sessions[session_id]
    
    def ask(self, query: str, session_id: str = None, use_reranking: bool = None) -> Dict:
        start_time = time.time()

        if use_reranking is None:
            use_reranking = self.enable_reranking

        session = self._get_or_create_session(session_id)

        try:
            query_classification = QueryClassifier.classify(query)

            # Bulletproof memory detection ‚Äî catches any ordinal + "question" combo
            is_memory = MemoryQueryDetector.is_memory_query(query)
            if not is_memory:
                ordinals = [
                    'first','second','third','fourth','fifth',
                    'sixth','seventh','eighth','ninth','tenth',
                    '1st','2nd','3rd','4th','5th',
                    '6th','7th','8th','9th','10th',
                    'last','previous','latest'
                ]
                q_lower = query.lower()
                if any(o in q_lower for o in ordinals) and 'question' in q_lower:
                    is_memory = True

            query_classification['is_memory_query'] = is_memory

            logger.info(
                f"[{session_id}] Query: {query[:50]}... | "
                f"Intent: {query_classification['intent']} | "
                f"Domain: {query_classification['is_domain_relevant']}"
            )

            if query_classification['is_memory_query']:
                response = self._handle_memory_query(query, query_classification, session)
            elif query_classification['is_domain_relevant']:
                response = self._handle_domain_query(query, query_classification, session, use_reranking)
            else:
                response = self._handle_general_query(query, query_classification, session)

            latency_ms = int((time.time() - start_time) * 1000)
            response['latency_ms'] = latency_ms

            session.update_health_stats(success=True, latency_ms=latency_ms)
            session.memory.add_turn(query, response['answer'], response)
            self._log_query(query, query_classification, response, session)
            self.flush_manager.increment_and_flush_if_needed()

            return response

        except Exception as e:
            logger.error(f"[{session_id}] Query processing failed: {e}", exc_info=True)
            latency_ms = int((time.time() - start_time) * 1000)
            session.update_health_stats(success=False, latency_ms=latency_ms)

            error_response = self._create_error_response(
                query,
                query_classification if 'query_classification' in locals() else {},
                str(e),
                latency_ms
            )
            session.memory.add_turn(query, error_response['answer'], error_response)

            try:
                self._log_query(query, query_classification, error_response, session)
                self.flush_manager.increment_and_flush_if_needed()
            except Exception as log_err:
                logger.error(f"Error logging failed: {log_err}")

            return error_response
    
    def _handle_memory_query(self, query: str, classification: Dict, session: SessionComponents) -> Dict:
        answer = MemoryResponseGenerator.handle_memory_query(query, session.memory)
        return {
            'answer':               answer,
            'citations':            [],
            'support_level':        'memory_recall',
            'confidence':           1.0,
            'retrieved_chunks':     [],
            'query_classification': classification,
            'error':                None
        }
    
    def _handle_domain_query(
        self, query: str, classification: Dict,
        session: SessionComponents, use_reranking: bool
    ) -> Dict:
        retrieved_chunks = self.retriever.retrieve(
            query, classification,
            top_k=self.config.FINAL_CONTEXT_K,
            use_reranking=use_reranking
        )
        
        if not retrieved_chunks:
            return {
                'answer':               "I couldn't find relevant information in my knowledge base.",
                'citations':            [],
                'support_level':        'not_supported',
                'confidence':           0.0,
                'retrieved_chunks':     [],
                'query_classification': classification,
                'error':                None
            }
        
        generation_result = self.generator.generate(query, retrieved_chunks, classification)
        
        return {
            'answer':               generation_result['answer'],
            'citations':            generation_result['citations'],
            'support_level':        generation_result['support_level'],
            'confidence':           generation_result['confidence'],
            'retrieved_chunks':     retrieved_chunks,
            'query_classification': classification,
            'grounding_scores':     generation_result.get('grounding_scores', {}),
            'error':                generation_result.get('error')
        }
    
    def _handle_general_query(self, query: str, classification: Dict, session: SessionComponents) -> Dict:
        """Handle general queries ‚Äî routes adjacent CVIP topics to domain handler,
        answers everything else via LLM general knowledge with caching."""
        
        # Route CVIP-adjacent queries to domain handler
        adjacent_terms = ['image', 'vision', 'visual', 'pixel', 'camera', 'detection']
        is_adjacent    = any(t in query.lower() for t in adjacent_terms)
        if is_adjacent:
            return self._handle_domain_query(query, classification, session, False)
        
        # Check general knowledge cache first
        cache_key = query.lower().strip()
        if cache_key in self._general_cache:
            logger.info(f"Cache hit for general query: {query[:40]}")
            return {
                'answer':               self._general_cache[cache_key],
                'citations':            [],
                'support_level':        'general_knowledge',
                'confidence':           0.8,
                'retrieved_chunks':     [],
                'query_classification': classification,
                'error':                None
            }
        
        try:
            answer = self._answer_general_query(query)
            self._general_cache[cache_key] = answer  # Cache for future
        except Exception:
            answer = (
                "I'm specialized in Computer Vision and Image Processing. "
                "Please ask about topics like CNNs, edge detection, segmentation, "
                "or computer vision algorithms."
            )
        
        return {
            'answer':               answer,
            'citations':            [],
            'support_level':        'general_knowledge',
            'confidence':           0.8,
            'retrieved_chunks':     [],
            'query_classification': classification,
            'error':                None
        }
    
    def _answer_general_query(self, query: str) -> str:
        """Direct LLM call for general knowledge ‚Äî 400 tokens max for speed."""
        import requests
        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        r = requests.post(
            f"{self.config.WORKSPACE_URL}/serving-endpoints/{self.config.LLM_ENDPOINT}/invocations",
            headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json"},
            json={
                "messages": [
                    {
                        "role": "system",
                        "content": (
                            "You are a knowledgeable assistant. Answer the user's question "
                            "clearly and accurately. For factual questions, provide precise "
                            "information. For technical questions, give detailed explanations "
                            "with examples where helpful."
                        )
                    },
                    {"role": "user", "content": query}
                ],
                "max_tokens": 400,
                "temperature": 0.3,
                "stream": False
            },
            timeout=120
        )
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"]
    
    def _create_error_response(
        self, query: str, classification: Dict, error_msg: str, latency_ms: int
    ) -> Dict:
        return {
            'answer':               "I encountered an error. Please try rephrasing your question.",
            'citations':            [],
            'support_level':        'error',
            'confidence':           0.0,
            'retrieved_chunks':     [],
            'query_classification': classification,
            'latency_ms':           latency_ms,
            'error':                error_msg
        }
    
    def _log_query(
        self, query: str, classification: Dict,
        response: Dict, session: SessionComponents
    ):
        try:
            self.logger.log_query(
                session_id=session.memory.session_id,
                query=query,
                query_classification=classification,
                retrieved_chunks=response.get('retrieved_chunks', []),
                generation_result=response,
                latency_ms=response['latency_ms'],
                error=response.get('error')
            )
        except Exception as e:
            logger.error(f"Logging failed: {e}")
    
    def display_response(self, response: Dict, show_debug: bool = False):
        print("\n" + "=" * 70)
        print("ü§ñ ANSWER")
        print("=" * 70)
        print(response['answer'])
        
        if response.get('citations') and response.get('query_classification', {}).get('is_domain_relevant'):
            print("\nüìö SOURCES:")
            for i, citation in enumerate(response['citations'], 1):
                print(f"   {i}. {citation}")
        
        if response['support_level'] not in ['memory_recall', 'out_of_domain', 'error']:
            print(f"\nüìä CONFIDENCE:")
            print(f"   Support: {response['support_level']}")
            print(f"   Score:   {response['confidence']:.3f}")
        
        print(f"\n‚ö° PERFORMANCE:")
        print(f"   Latency: {response.get('latency_ms', 0)} ms")
        
        if response.get('error'):
            print(f"\n‚ö†Ô∏è  ERROR: {response['error']}")
        
        print("=" * 70)
    
    def get_system_stats(self) -> Dict:
        uptime = datetime.now() - self.system_stats['system_start']
        return {
            'system_uptime':      str(uptime).split('.')[0],
            'total_sessions':     self.system_stats['total_sessions'],
            'active_sessions':    self.system_stats['active_sessions'],
            'cache_stats':        self.metadata_cache.get_stats(),
            'general_cache_size': len(self._general_cache)
        }
    
    def get_session_summary(self, session_id: str = None) -> Dict:
        session = self._get_or_create_session(session_id)
        self.flush_manager.force_flush()
        return {
            **session.memory.get_session_summary(),
            **session.get_health_stats(),
            **self.logger.get_session_analytics(session.memory.session_id)
        }
    
    def cleanup(self, session_id: str = None):
        logger.info(f"üßπ Cleaning up: {session_id or 'all'}")
        self.flush_manager.force_flush()
        if session_id:
            with self.sessions_lock:
                if session_id in self.sessions:
                    del self.sessions[session_id]
                    self.system_stats['active_sessions'] = len(self.sessions)
        logger.info("‚úÖ Cleanup complete")


print("‚úÖ FinalProductionRAG ready")
print("   ‚úî General knowledge cache ‚Äî repeated queries return instantly")
print("   ‚úî CVIP-adjacent queries routed to domain handler automatically")
print("   ‚úî max_tokens=400 for general queries ‚Äî faster responses")
print("   ‚úî session passed correctly to _handle_general_query")
print("   ‚úî all syntax errors fixed")

# COMMAND ----------

import os

# Required
os.environ["DATABRICKS_HOST"] = "https://YOUR_WORKSPACE.cloud.databricks.com"

# Usually optional inside Databricks notebook runtime, but set if your client requires explicit PAT:
# os.environ["DATABRICKS_TOKEN"] = dbutils.secrets.get("your-scope", "your-token-key")


# COMMAND ----------

import os
print("HOST set:", bool(os.getenv("DATABRICKS_HOST")))
print("TOKEN set:", bool(os.getenv("DATABRICKS_TOKEN")))


# COMMAND ----------

# Check available serving endpoints in your workspace
from databricks.sdk import WorkspaceClient

w = WorkspaceClient()
endpoints = w.serving_endpoints.list()
for ep in endpoints:
    print(f"  {ep.name}  ‚Äî  state: {ep.state.ready if ep.state else 'unknown'}")

# COMMAND ----------

print("üß™ Testing CVIP RAG System")
print("=" * 70)

try:
    print("üöÄ Initializing RAG system...")
    rag = FinalProductionRAG(
        enable_reranking=False,
        enable_persistence=True,
        flush_every_n=3,
        flush_every_seconds=30
    )
    
    print("\n‚úÖ System initialized successfully!")
    print("=" * 70)
    
    print("\nüîç Running test query...")
    test_query = "What is edge detection in image processing?"
    
    response = rag.ask(
        query=test_query,
        session_id="test_session",
        use_reranking=False
    )
    
    # Debug lines ‚Äî inside try block, correct indentation
    print("RAW RESPONSE KEYS:", list(response.keys()))
    print("ANSWER:", response.get('answer', 'NO ANSWER KEY'))
    print("ERROR:", response.get('error', 'NO ERROR'))
    print("SUPPORT:", response.get('support_level', 'NONE'))
    
    rag.display_response(response, show_debug=True)
    
    health = rag.get_system_stats()
    print(f"\nüìà Active Sessions: {health.get('active_sessions', 0)}")
    print(f"   Cache Size: {health.get('cache_stats', {}).get('cache_size', 0)}")
    
    print("\nüéâ Your RAG system is fully operational!")

except Exception as e:
    print(f"\n‚ùå Test failed: {e}")
    import traceback
    traceback.print_exc()

# COMMAND ----------

import os, inspect

os.makedirs("/tmp/cvip_app", exist_ok=True)

# Save rag_components.py
classes = [
    'SecureConfig', 'MetadataCache', 'EnhancedMetadataCache',
    'SafeMetadataFetcher', 'SafeResultParser', 'QueryClassifier',
    'MemoryQueryDetector', 'MemoryResponseGenerator', '_OfflineTokenizer',
    'ProductionMemorySystem', 'ImprovedGroundingChecker', 'FixedAnswerGenerator',
    'FinalQueryLogger', 'CrossEncoderManager', 'SmartFlushManager',
    'SessionComponents', 'FinalProductionRAG'
]

with open("/tmp/cvip_app/rag_components.py", "w") as f:
    f.write("""import re, uuid, time, logging, threading, os, atexit
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import deque, defaultdict
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import *
from databricks.vector_search.client import VectorSearchClient
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
""")
    for cls_name in classes:
        cls = globals().get(cls_name)
        if cls:
            try:
                f.write(inspect.getsource(cls) + "\n\n")
                print(f"‚úÖ {cls_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Skipped {cls_name}: {e}")

print("\n‚úÖ rag_components.py saved")

# COMMAND ----------

import os
os.makedirs("/tmp/cvip_app", exist_ok=True)

# Read source directly from your notebook cells
# Since inspect won't work, we copy the files that were already saved

import shutil

# The components are already defined in this notebook's memory
# Write them by reading the actual cell files
cell_files = [
    f for f in os.listdir("/home/spark-c33605bd-0e8a-460c-a259-df/.ipykernel/")
    if f.endswith(".py")
]
print("Available kernel files:", cell_files[:5])

# Better approach ‚Äî write rag_components.py by referencing the command files
import glob
kernel_dir = glob.glob("/home/spark-*/.ipykernel/*/")[0]
print(f"Kernel dir: {kernel_dir}")

# Find our component cell (the big one with all classes)
cmd_files = sorted(glob.glob(f"{kernel_dir}command-6355831562865777-*.py") +
                   glob.glob(f"{kernel_dir}command-4783263896246212-*.py"))
print(f"Found command files: {cmd_files}")

if cmd_files:
    with open("/tmp/cvip_app/rag_components.py", "w") as out:
        out.write("import os\nos.environ.setdefault('DATABRICKS_HOST','https://YOUR_WORKSPACE.cloud.databricks.com')\n\n")
        for f in cmd_files:
            out.write(open(f).read() + "\n\n")
    print("‚úÖ rag_components.py saved from kernel files")
else:
    print("‚ùå Could not find kernel files ‚Äî trying alternate path")
    # List what's in the kernel dir
    print(os.listdir(kernel_dir)[:20])

# COMMAND ----------

import os
os.makedirs("/tmp/cvip_app", exist_ok=True)

# Read the actual cell source from Databricks notebook API
import json
ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()
token = ctx.apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"
path  = ctx.notebookPath().get()

import requests
r = requests.get(
    f"{host}/api/2.0/workspace/export",
    headers={"Authorization": f"Bearer {token}"},
    params={"path": path, "format": "SOURCE"}
)

if r.status_code == 200:
    import base64
    source = base64.b64decode(r.json()["content"]).decode("utf-8")
    
    with open("/tmp/cvip_app/rag_components.py", "w") as f:
        f.write(source)
    
    print(f"‚úÖ Saved {len(source)} chars to rag_components.py")
    print("First 200 chars:", source[:200])
else:
    print(f"‚ùå Failed: {r.status_code} {r.text}")

# COMMAND ----------

# Write the Streamlit app
with open("/tmp/cvip_app/cvip_app.py", "w") as f:
    f.write('''
import streamlit as st, time, sys
from datetime import datetime
sys.path.insert(0, "/tmp/cvip_app")

st.set_page_config(page_title="CVIP RAG", page_icon="ü§ñ", layout="wide")
st.markdown("""<style>
.answer-box{background:#f5f7fa;padding:1.5rem;border-radius:12px;
border-left:5px solid #1E88E5;margin:1rem 0;line-height:1.7;}
.citation-box{background:#fff9e6;padding:.75rem;border-radius:8px;
border-left:3px solid #ffc107;margin:.3rem 0;font-size:.9rem;}
</style>""", unsafe_allow_html=True)

if "chat_history" not in st.session_state: st.session_state.chat_history=[]
if "session_id"   not in st.session_state: st.session_state.session_id=f"ui_{int(time.time())}"
if "initialized"  not in st.session_state: st.session_state.initialized=False
if "rag"          not in st.session_state: st.session_state.rag=None

@st.cache_resource
def load_rag():
    import sys, os
    sys.path.insert(0, "/tmp/cvip_app")
    os.environ.setdefault("DATABRICKS_HOST","https://YOUR_WORKSPACE.cloud.databricks.com")
    
    # Execute the notebook source to get all class definitions
    with open("/tmp/cvip_app/rag_components.py") as f:
        source = f.read()
    
    namespace = {"__name__": "__main__"}
    exec(compile(source, "rag_components.py", "exec"), namespace)
    
    FinalProductionRAG = namespace["FinalProductionRAG"]
    return FinalProductionRAG(
        enable_reranking=False,
        enable_persistence=True,
        flush_every_n=3,
        flush_every_seconds=30
    )

with st.sidebar:
    st.title("‚öôÔ∏è Controls")
    if not st.session_state.initialized:
        if st.button("üöÄ Initialize System", use_container_width=True):
            with st.spinner("Loading‚Ä¶ (2-3 min first time)"):
                try:
                    st.session_state.rag = load_rag()
                    st.session_state.initialized = True
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå {e}")
    else:
        st.success("‚úÖ System Online")
    st.markdown("---")
    show_sources = st.checkbox("üìö Show Sources", value=True)
    show_debug   = st.checkbox("üîç Debug Info",   value=False)
    st.markdown("---")
    if st.button("üîÑ New Chat", use_container_width=True):
        if st.session_state.rag:
            try: st.session_state.rag.cleanup(st.session_state.session_id)
            except: pass
        st.session_state.chat_history = []
        st.session_state.session_id   = f"ui_{int(time.time())}"
        st.rerun()
    if st.session_state.initialized and st.session_state.rag:
        st.markdown("---")
        st.markdown("### üìä Stats")
        try:
            s = st.session_state.rag.get_system_stats()
            st.metric("Sessions", s.get("active_sessions", 0))
            st.metric("Uptime",   s.get("system_uptime",   "-"))
        except: pass

st.markdown("# ü§ñ CVIP RAG System")
st.markdown("*Computer Vision & Image Processing Expert*")
st.markdown("---")

if not st.session_state.initialized:
    st.info("üëà Click **Initialize System** in the sidebar to begin.")
    st.stop()

EXAMPLES = ["What is edge detection?","How does Sobel work?",
            "Explain CNNs","What are vision transformers?","Compare CNN vs traditional CV"]

if not st.session_state.chat_history:
    st.markdown("### üí° Example Questions")
    cols = st.columns(len(EXAMPLES))
    for col, ex in zip(cols, EXAMPLES):
        with col:
            if st.button(ex, use_container_width=True):
                st.session_state.pending = ex
                st.rerun()
    st.markdown("---")

for entry in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(entry["query"])
        st.caption(entry["time"])
    with st.chat_message("assistant"):
        r = entry["response"]
        st.markdown(f\'<div class="answer-box">{r["answer"]}</div>\', unsafe_allow_html=True)
        if show_sources and r.get("citations"):
            with st.expander("üìö Sources"):
                for i,c in enumerate(r["citations"],1):
                    st.markdown(f\'<div class="citation-box">{i}. {c}</div>\', unsafe_allow_html=True)
        if r["support_level"] not in ["memory_recall","out_of_domain","error"]:
            c1,c2,c3 = st.columns(3)
            c1.metric("Support",    r["support_level"])
            c2.metric("Confidence", f\'{r["confidence"]:.1%}\')
            c3.metric("Latency",    f\'{r["latency_ms"]}ms\')
        if show_debug:
            with st.expander("üêõ Debug"):
                st.json({"intent":   r["query_classification"]["intent"],
                         "chunks":   len(r.get("retrieved_chunks",[])),
                         "grounding":r.get("grounding_scores",{})})

query = None
if hasattr(st.session_state, "pending"):
    query = st.session_state.pending
    del st.session_state.pending
else:
    query = st.chat_input("Ask about Computer Vision or Image Processing‚Ä¶")

if query:
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        with st.spinner("Thinking‚Ä¶"):
            try:
                r = st.session_state.rag.ask(
                    query=query,
                    session_id=st.session_state.session_id,
                    use_reranking=False
                )
                st.session_state.chat_history.append({
                    "query": query, "response": r,
                    "time":  datetime.now().strftime("%I:%M %p")
                })
                st.rerun()
            except Exception as e:
                st.error(f"‚ùå {e}")

st.markdown("---")
st.caption("ü§ñ CVIP RAG | Databricks Vector Search + LLaMA 3.3 70B")
''')

print("‚úÖ cvip_app.py saved")
print("\nüöÄ Now open Databricks Web Terminal and run:")
print("   streamlit run /tmp/cvip_app/cvip_app.py --server.port 8501")

# COMMAND ----------

import os, requests, base64
os.makedirs("/tmp/cvip_app", exist_ok=True)

# Already have rag_components.py from before ‚Äî just create the app.yaml and app.py
# For Databricks Apps, we need a specific structure

# Create app.py
with open("/tmp/cvip_app/app.py", "w") as f:
    f.write(open("/tmp/cvip_app/cvip_app.py").read())

# Create app.yaml (required by Databricks Apps)
with open("/tmp/cvip_app/app.yaml", "w") as f:
    f.write("""
command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]
""")

print("‚úÖ Files ready")
print("üìÅ /tmp/cvip_app/app.py")
print("üìÅ /tmp/cvip_app/app.yaml")

# COMMAND ----------

import requests, base64

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"

def upload_to_workspace(content, path):
    r = requests.post(
        f"{host}/api/2.0/workspace/import",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "path": path,
            "format": "AUTO",
            "content": base64.b64encode(content.encode()).decode(),
            "overwrite": True
        }
    )
    return r.status_code, r.text

# Upload app.py
app_code = open("/tmp/cvip_app/cvip_app.py").read()
status, msg = upload_to_workspace(app_code, "/cvip_rag_app/app.py")
print(f"app.py: {status} {msg}")

# Upload app.yaml
yaml_code = '''command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]'''
status, msg = upload_to_workspace(yaml_code, "/cvip_rag_app/app.yaml")
print(f"app.yaml: {status} {msg}")

print("\n‚úÖ Files uploaded to workspace at /cvip_rag_app/")

# COMMAND ----------

import requests, base64

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"

def upload(content, path):
    r = requests.post(
        f"{host}/api/2.0/workspace/import",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "path": path,
            "format": "AUTO", 
            "content": base64.b64encode(content.encode()).decode(),
            "overwrite": True
        }
    )
    print(f"{path}: {r.status_code}")
    return r.status_code

# app.yaml ‚Äî tells Databricks how to run the app
upload(
    'command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]',
    "/cvip_rag_app/app.yaml"
)

# app.py ‚Äî the Streamlit UI
upload(open("/tmp/cvip_app/cvip_app.py").read(), "/cvip_rag_app/app.py")

# rag_components.py ‚Äî all your RAG classes
upload(open("/tmp/cvip_app/rag_components.py").read(), "/cvip_rag_app/rag_components.py")

print("\n‚úÖ All files uploaded to /cvip_rag_app/")

# COMMAND ----------

import requests, base64

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"
headers = {"Authorization": f"Bearer {token}"}

# Step 1: Create the folder first
r = requests.post(
    f"{host}/api/2.0/workspace/mkdirs",
    headers=headers,
    json={"path": "/cvip_rag_app"}
)
print(f"mkdir: {r.status_code} {r.text}")

# Step 2: Upload files
def upload(content, path):
    r = requests.post(
        f"{host}/api/2.0/workspace/import",
        headers=headers,
        json={
            "path": path,
            "format": "AUTO",
            "content": base64.b64encode(content.encode()).decode(),
            "overwrite": True
        }
    )
    print(f"{'‚úÖ' if r.status_code==200 else '‚ùå'} {path}: {r.status_code} {r.text}")

upload(
    'command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]',
    "/cvip_rag_app/app.yaml"
)
upload(open("/tmp/cvip_app/cvip_app.py").read(), "/cvip_rag_app/app.py")
upload(open("/tmp/cvip_app/rag_components.py").read(), "/cvip_rag_app/rag_components.py")

# COMMAND ----------

import requests, base64

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"

def upload(content, path):
    r = requests.post(
        f"{host}/api/2.0/workspace/import",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "path": path,
            "format": "AUTO",
            "content": base64.b64encode(content.encode()).decode(),
            "overwrite": True
        }
    )
    print(f"{'‚úÖ' if r.status_code==200 else '‚ùå'} {path}: {r.status_code}")

# 1. app.yaml
upload(
    'command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]',
    "/cvip_rag_app/app.yaml"
)

# 2. app.py (Streamlit UI)
upload(open("/tmp/cvip_app/cvip_app.py").read(), "/cvip_rag_app/app.py")

# 3. rag_components.py (all RAG classes)
upload(open("/tmp/cvip_app/rag_components.py").read(), "/cvip_rag_app/rag_components.py")

print("\n‚úÖ Done ‚Äî now go to your app and click Deploy")

# COMMAND ----------

import os, base64, requests

os.makedirs("/tmp/cvip_app", exist_ok=True)

# Step 1: Re-export notebook as rag_components.py
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"
path  = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()

r = requests.get(
    f"{host}/api/2.0/workspace/export",
    headers={"Authorization": f"Bearer {token}"},
    params={"path": path, "format": "SOURCE"}
)
source = base64.b64decode(r.json()["content"]).decode("utf-8")
with open("/tmp/cvip_app/rag_components.py", "w") as f:
    f.write(source)
print(f"‚úÖ rag_components.py: {len(source)} chars")

# Step 2: Write app.yaml
with open("/tmp/cvip_app/app.yaml", "w") as f:
    f.write('command: ["streamlit", "run", "app.py", "--server.port", "8080", "--server.address", "0.0.0.0"]\n')
print("‚úÖ app.yaml created")

# Step 3: Write app.py (Streamlit UI)
with open("/tmp/cvip_app/app.py", "w") as f:
    f.write('''
import streamlit as st, time, sys, os
from datetime import datetime
sys.path.insert(0, "/tmp/cvip_app")

st.set_page_config(page_title="CVIP RAG", page_icon="ü§ñ", layout="wide")
st.markdown("""<style>
.answer-box{background:#f5f7fa;padding:1.5rem;border-radius:12px;
border-left:5px solid #1E88E5;margin:1rem 0;line-height:1.7;}
.citation-box{background:#fff9e6;padding:.75rem;border-radius:8px;
border-left:3px solid #ffc107;margin:.3rem 0;font-size:.9rem;}
</style>""", unsafe_allow_html=True)

if "chat_history" not in st.session_state: st.session_state.chat_history=[]
if "session_id"   not in st.session_state: st.session_state.session_id=f"ui_{int(time.time())}"
if "initialized"  not in st.session_state: st.session_state.initialized=False
if "rag"          not in st.session_state: st.session_state.rag=None

@st.cache_resource
def load_rag():
    os.environ.setdefault("DATABRICKS_HOST","https://YOUR_WORKSPACE.cloud.databricks.com")
    with open("/tmp/cvip_app/rag_components.py") as f:
        source = f.read()
    namespace = {"__name__": "__main__"}
    exec(compile(source, "rag_components.py", "exec"), namespace)
    FinalProductionRAG = namespace["FinalProductionRAG"]
    return FinalProductionRAG(
        enable_reranking=False, enable_persistence=True,
        flush_every_n=3, flush_every_seconds=30
    )

with st.sidebar:
    st.title("‚öôÔ∏è Controls")
    if not st.session_state.initialized:
        if st.button("üöÄ Initialize System", use_container_width=True):
            with st.spinner("Loading‚Ä¶ (2-3 min first time)"):
                try:
                    st.session_state.rag = load_rag()
                    st.session_state.initialized = True
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå {e}")
    else:
        st.success("‚úÖ System Online")
    st.markdown("---")
    show_sources = st.checkbox("üìö Show Sources", value=True)
    show_debug   = st.checkbox("üîç Debug Info",   value=False)
    st.markdown("---")
    if st.button("üîÑ New Chat", use_container_width=True):
        if st.session_state.rag:
            try: st.session_state.rag.cleanup(st.session_state.session_id)
            except: pass
        st.session_state.chat_history = []
        st.session_state.session_id   = f"ui_{int(time.time())}"
        st.rerun()
    if st.session_state.initialized and st.session_state.rag:
        st.markdown("---")
        try:
            s = st.session_state.rag.get_system_stats()
            st.metric("Sessions", s.get("active_sessions",0))
            st.metric("Uptime",   s.get("system_uptime","-"))
        except: pass

st.markdown("# ü§ñ CVIP RAG System")
st.markdown("*Computer Vision & Image Processing Expert*")
st.markdown("---")

if not st.session_state.initialized:
    st.info("üëà Click **Initialize System** in the sidebar to begin.")
    st.stop()

EXAMPLES=["What is edge detection?","How does Sobel work?",
          "Explain CNNs","What are vision transformers?","Compare CNN vs traditional CV"]

if not st.session_state.chat_history:
    st.markdown("### üí° Example Questions")
    cols=st.columns(len(EXAMPLES))
    for col,ex in zip(cols,EXAMPLES):
        with col:
            if st.button(ex, use_container_width=True):
                st.session_state.pending=ex
                st.rerun()
    st.markdown("---")

for entry in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(entry["query"])
        st.caption(entry["time"])
    with st.chat_message("assistant"):
        r=entry["response"]
        st.markdown(f\'<div class="answer-box">{r["answer"]}</div>\', unsafe_allow_html=True)
        if show_sources and r.get("citations"):
            with st.expander("üìö Sources"):
                for i,c in enumerate(r["citations"],1):
                    st.markdown(f\'<div class="citation-box">{i}. {c}</div>\', unsafe_allow_html=True)
        if r["support_level"] not in ["memory_recall","out_of_domain","error"]:
            c1,c2,c3=st.columns(3)
            c1.metric("Support",    r["support_level"])
            c2.metric("Confidence", f\'{r["confidence"]:.1%}\')
            c3.metric("Latency",    f\'{r["latency_ms"]}ms\')
        if show_debug:
            with st.expander("üêõ Debug"):
                st.json({"intent":r["query_classification"]["intent"],
                         "chunks":len(r.get("retrieved_chunks",[])),
                         "grounding":r.get("grounding_scores",{})})

query=None
if hasattr(st.session_state,"pending"):
    query=st.session_state.pending
    del st.session_state.pending
else:
    query=st.chat_input("Ask about Computer Vision or Image Processing‚Ä¶")

if query:
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        with st.spinner("Thinking‚Ä¶"):
            try:
                r=st.session_state.rag.ask(
                    query=query,
                    session_id=st.session_state.session_id,
                    use_reranking=False
                )
                st.session_state.chat_history.append({
                    "query":query,"response":r,
                    "time":datetime.now().strftime("%I:%M %p")
                })
                st.rerun()
            except Exception as e:
                st.error(f"‚ùå {e}")

st.markdown("---")
st.caption("ü§ñ CVIP RAG | Databricks Vector Search + LLaMA 3.3 70B")
''')
print("‚úÖ app.py created")

# Step 4: Push all 3 files to GitHub
GITHUB_TOKEN = "GITHUB_TOKEN_REMOVED"  # ‚Üê paste your token here
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

def push(filename, local_path):
    with open(local_path, "rb") as f:
        content = base64.b64encode(f.read()).decode()
    r = requests.get(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"}
    )
    sha = r.json().get("sha") if r.status_code == 200 else None
    payload = {"message": f"Add {filename}", "content": content, "branch": BRANCH}
    if sha:
        payload["sha"] = sha
    r = requests.put(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"},
        json=payload
    )
    print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} {filename}: {r.status_code}")

push("app.yaml",          "/tmp/cvip_app/app.yaml")
push("app.py",            "/tmp/cvip_app/app.py")
push("rag_components.py", "/tmp/cvip_app/rag_components.py")

print("\nüéâ All done! Now go to cvip-rag app ‚Üí Deploy")

# COMMAND ----------

import requests, base64

GITHUB_TOKEN = "GITHUB_TOKEN_REMOVED"
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

# Get existing SHA first
r = requests.get(
    f"https://api.github.com/repos/{REPO}/contents/rag_components.py",
    headers={"Authorization": f"token {GITHUB_TOKEN}"}
)
print(f"Existing file check: {r.status_code}")
sha = r.json().get("sha") if r.status_code == 200 else None
print(f"SHA: {sha}")

# Read and encode
with open("/tmp/cvip_app/rag_components.py", "rb") as f:
    content = base64.b64encode(f.read()).decode()

print(f"Content size: {len(content)} chars")

# Push with SHA
payload = {
    "message": "Add rag_components.py",
    "content": content,
    "branch":  BRANCH
}
if sha:
    payload["sha"] = sha

r = requests.put(
    f"https://api.github.com/repos/{REPO}/contents/rag_components.py",
    headers={
        "Authorization": f"token {GITHUB_TOKEN}",
        "Content-Type":  "application/json"
    },
    json=payload,
    timeout=120  # large file needs more time
)
print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} rag_components.py: {r.status_code}")
if r.status_code not in [200, 201]:
    print(r.text[:300])

# COMMAND ----------

import re, requests, base64

GITHUB_TOKEN = "GITHUB_TOKEN_REMOVED"
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

# Read and scrub secrets from rag_components.py
with open("/tmp/cvip_app/rag_components.py", "r") as f:
    source = f.read()

# Remove any hardcoded tokens/URLs that GitHub flags
source = re.sub(r'dapi[a-zA-Z0-9]{32,}', 'DATABRICKS_TOKEN_REMOVED', source)
source = re.sub(r'https://dbc-[a-zA-Z0-9\-]+\.cloud\.databricks\.com', 
                'https://YOUR_WORKSPACE.cloud.databricks.com', source)

# Save scrubbed version
with open("/tmp/cvip_app/rag_components_clean.py", "w") as f:
    f.write(source)
print(f"‚úÖ Scrubbed file: {len(source)} chars")

# Push clean version
content = base64.b64encode(source.encode()).decode()
r = requests.put(
    f"https://api.github.com/repos/{REPO}/contents/rag_components.py",
    headers={"Authorization": f"token {GITHUB_TOKEN}"},
    json={
        "message": "Add rag_components.py (secrets removed)",
        "content": content,
        "branch":  BRANCH
    },
    timeout=120
)
print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} rag_components.py: {r.status_code}")
if r.status_code not in [200, 201]:
    print(r.text[:300])

# COMMAND ----------

import re, requests, base64

GITHUB_TOKEN = "GITHUB_TOKEN_REMOVED"
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

with open("/tmp/cvip_app/rag_components.py", "r") as f:
    source = f.read()

# Remove ALL patterns GitHub flags
patterns = [
    (r'dapi[a-zA-Z0-9]{32,}',                          'DATABRICKS_TOKEN_REMOVED'),
    (r'ghp_[a-zA-Z0-9]{36,}',                          'GITHUB_TOKEN_REMOVED'),
    (r'github_pat_[a-zA-Z0-9_]{80,}',                  'GITHUB_TOKEN_REMOVED'),
    (r'https://dbc-[a-zA-Z0-9\-]+\.cloud\.databricks\.com', 'https://YOUR_WORKSPACE.cloud.databricks.com'),
    (r'[a-zA-Z0-9]{8}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{4}-[a-zA-Z0-9]{12}', 'UUID_REMOVED'),
]

for pattern, replacement in patterns:
    matches = re.findall(pattern, source)
    if matches:
        print(f"Found {len(matches)} matches for: {pattern[:40]}")
        print(f"  Example: {matches[0][:60]}")
    source = re.sub(pattern, replacement, source)

# Also find anything that looks like a token GitHub might flag
suspicious = re.findall(r'["\']([A-Za-z0-9_\-]{40,})["\']', source)
for s in suspicious[:10]:
    print(f"‚ö†Ô∏è  Possible secret: {s[:30]}...")

with open("/tmp/cvip_app/rag_components_clean.py", "w") as f:
    f.write(source)
print(f"\n‚úÖ Cleaned file: {len(source)} chars")

# Push
content = base64.b64encode(source.encode()).decode()
r = requests.put(
    f"https://api.github.com/repos/{REPO}/contents/rag_components.py",
    headers={"Authorization": f"token {GITHUB_TOKEN}"},
    json={
        "message": "Add rag_components (all secrets removed)",
        "content": content,
        "branch":  BRANCH
    },
    timeout=120
)
print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} rag_components.py: {r.status_code}")
if r.status_code not in [200, 201]:
    print(r.text[:500])

# COMMAND ----------

import getpass
GITHUB_TOKEN = getpass.getpass("GITHUB_TOKEN_REMOVED ")  # hidden input, not saved in cell

# COMMAND ----------

import requests, base64

# This creates a text input widget ‚Äî token never touches the cell code
token_widget = dbutils.widgets.text("github_token", "", "Paste GitHub Token")

# COMMAND ----------

dbutils.widgets.get("github_token")

# COMMAND ----------

import requests, base64, getpass

GITHUB_TOKEN = getpass.getpass("New token: ")
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

requirements = """streamlit>=1.28.0
databricks-vectorsearch>=0.22
databricks-sdk>=0.12.0
mlflow>=2.9.0
pyspark>=3.4.0
openai>=1.0.0
requests>=2.31.0
"""

content = base64.b64encode(requirements.encode()).decode()
r = requests.put(
    f"https://api.github.com/repos/{REPO}/contents/requirements.txt",
    headers={"Authorization": f"token {GITHUB_TOKEN}"},
    json={
        "message": "Add requirements.txt",
        "content": content,
        "branch":  BRANCH
    }
)
print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} requirements.txt: {r.status_code}")

# COMMAND ----------

# Check what the app sees when it tries to load
import subprocess
result = subprocess.run(
    ["pip", "show", "streamlit"],
    capture_output=True, text=True
)
print(result.stdout)

# COMMAND ----------

import requests, base64, getpass

GITHUB_TOKEN = getpass.getpass("Token: ")
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

app_code = '''
import streamlit as st
import requests
import time
import os
from datetime import datetime

st.set_page_config(page_title="CVIP RAG", page_icon="ü§ñ", layout="wide")
st.markdown("""<style>
.answer-box{background:#f5f7fa;padding:1.5rem;border-radius:12px;
border-left:5px solid #1E88E5;margin:1rem 0;line-height:1.7;}
.citation-box{background:#fff9e6;padding:.75rem;border-radius:8px;
border-left:3px solid #ffc107;margin:.3rem 0;font-size:.9rem;}
</style>""", unsafe_allow_html=True)

if "chat_history" not in st.session_state: st.session_state.chat_history=[]
if "session_id"   not in st.session_state: st.session_state.session_id=f"ui_{int(time.time())}"

DATABRICKS_HOST  = os.environ.get("DATABRICKS_HOST",  "https://YOUR_WORKSPACE.cloud.databricks.com")
DATABRICKS_TOKEN = os.environ.get("DATABRICKS_TOKEN", "")
LLM_ENDPOINT     = "databricks-meta-llama-3-3-70b-instruct"
VECTOR_INDEX     = "workspace.default.cvip_chunks_vs_index"
VECTOR_ENDPOINT  = "cvip_endpoint"

def query_llm(query, context):
    import mlflow.deployments
    client = mlflow.deployments.get_deploy_client("databricks")
    response = client.predict(
        endpoint=LLM_ENDPOINT,
        inputs={
            "messages": [
                {"role": "system", "content": (
                    "You are an expert in Computer Vision and Image Processing. "
                    "Answer ONLY using the provided context. "
                    "Cite sources using [Source: name] format."
                )},
                {"role": "user", "content": f"Context:\\n{context}\\n\\nQuestion: {query}"}
            ],
            "max_tokens": 800,
            "temperature": 0.1,
            "stream": False
        }
    )
    return response["choices"][0]["message"]["content"]

def query_vector_search(query):
    from databricks.vector_search.client import VectorSearchClient
    vsc   = VectorSearchClient()
    index = vsc.get_index(
        endpoint_name=VECTOR_ENDPOINT,
        index_name=VECTOR_INDEX
    )
    results = index.similarity_search(
        query_text=query,
        columns=["chunk_id","content","citation_label","page_number"],
        num_results=5
    )
    chunks = []
    for row in results.get("result",{}).get("data_array",[]):
        chunks.append({
            "content":        row[1] if len(row)>1 else "",
            "citation_label": row[2] if len(row)>2 else "Unknown",
            "page_number":    row[3] if len(row)>3 else ""
        })
    return chunks

def ask(query):
    chunks  = query_vector_search(query)
    if not chunks:
        return {"answer": "No relevant information found.", "citations": [], "latency_ms": 0}
    context = "\\n\\n".join([
        f"[Source: {c['citation_label']}]\\n{c['content'][:500]}"
        for c in chunks
    ])
    start   = time.time()
    answer  = query_llm(query, context)
    latency = int((time.time()-start)*1000)
    import re
    citations = re.findall(r"\\[Source:([^\\]]+)\\]", answer)
    return {"answer": answer, "citations": citations, "latency_ms": latency, "chunks": len(chunks)}

# Sidebar
with st.sidebar:
    st.title("‚öôÔ∏è CVIP RAG")
    st.success("‚úÖ System Ready")
    st.markdown("---")
    show_sources = st.checkbox("üìö Show Sources", value=True)
    st.markdown("---")
    if st.button("üîÑ New Chat", use_container_width=True):
        st.session_state.chat_history=[]
        st.session_state.session_id=f"ui_{int(time.time())}"
        st.rerun()
    st.markdown("---")
    st.markdown("### üí° Examples")
    EXAMPLES=["What is edge detection?","How does Sobel work?",
              "Explain CNNs","What are vision transformers?"]
    for ex in EXAMPLES:
        if st.button(ex, use_container_width=True):
            st.session_state.pending=ex
            st.rerun()

st.markdown("# ü§ñ CVIP RAG System")
st.markdown("*Computer Vision & Image Processing Expert*")
st.markdown("---")

for entry in st.session_state.chat_history:
    with st.chat_message("user"):
        st.markdown(entry["query"])
        st.caption(entry["time"])
    with st.chat_message("assistant"):
        r=entry["response"]
        st.markdown(f\'<div class="answer-box">{r["answer"]}</div>\',unsafe_allow_html=True)
        if show_sources and r.get("citations"):
            with st.expander("üìö Sources"):
                for i,c in enumerate(r["citations"],1):
                    st.markdown(f\'<div class="citation-box">{i}. {c}</div>\',unsafe_allow_html=True)
        c1,c2=st.columns(2)
        c1.metric("Chunks Retrieved", r.get("chunks",0))
        c2.metric("Latency", f\'{r["latency_ms"]}ms\')

query=None
if hasattr(st.session_state,"pending"):
    query=st.session_state.pending
    del st.session_state.pending
else:
    query=st.chat_input("Ask about Computer Vision or Image Processing‚Ä¶")

if query:
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        with st.spinner("Thinking‚Ä¶"):
            try:
                r=ask(query)
                st.session_state.chat_history.append({
                    "query":query,"response":r,
                    "time":datetime.now().strftime("%I:%M %p")
                })
                st.rerun()
            except Exception as e:
                st.error(f"‚ùå {e}")

st.markdown("---")
st.caption("ü§ñ CVIP RAG | Databricks Vector Search + LLaMA 3.3 70B")
'''

# Push to GitHub
def push(filename, content):
    r = requests.get(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"}
    )
    sha = r.json().get("sha") if r.status_code==200 else None
    payload = {"message": f"Update {filename}", 
               "content": base64.b64encode(content.encode()).decode(),
               "branch": BRANCH}
    if sha: payload["sha"] = sha
    r = requests.put(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"},
        json=payload, timeout=30
    )
    print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} {filename}: {r.status_code}")

push("app.py", app_code)

# Minimal requirements ‚Äî no pyspark (provided by Databricks runtime)
push("requirements.txt", """streamlit>=1.28.0
databricks-vectorsearch>=0.22
mlflow>=2.9.0
""")

print("\n‚úÖ Done ‚Äî go to cvip-rag app and Deploy again")

# COMMAND ----------

import requests

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"

r = requests.get(
    f"{host}/api/2.0/apps/cvip-rag",
    headers={"Authorization": f"Bearer {token}"}
)
import json
print(json.dumps(r.json(), indent=2))

# COMMAND ----------

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"

import requests

# Create secret scope
r = requests.post(
    f"{host}/api/2.0/secrets/scopes/create",
    headers={"Authorization": f"Bearer {token}"},
    json={"scope": "cvip-rag-scope"}
)
print(f"Create scope: {r.status_code} {r.text}")

# Add Databricks token as secret
r = requests.post(
    f"{host}/api/2.0/secrets/put",
    headers={"Authorization": f"Bearer {token}"},
    json={
        "scope": "cvip-rag-scope",
        "key":   "DATABRICKS_TOKEN",
        "string_value": token  # uses current session token
    }
)
print(f"Add secret: {r.status_code} {r.text}")

print("\n‚úÖ Done ‚Äî now in the Authorization tab:")
print("   Scope: cvip-rag-scope")
print("   Key:   DATABRICKS_TOKEN")

# COMMAND ----------

# ============================================================
# CHECK DATABRICKS APP STATUS
# ============================================================

from databricks.sdk import WorkspaceClient

print("üîç Checking Databricks App status...")
print("=" * 70)

try:
    w = WorkspaceClient()
    
    # List all apps
    apps = w.apps.list()
    
    print("\nüì± Your Databricks Apps:")
    for app in apps:
        print(f"\n   Name: {app.name}")
        print(f"   Status: {app.status.state if hasattr(app, 'status') else 'Unknown'}")
        print(f"   URL: {app.url if hasattr(app, 'url') else 'N/A'}")
        
        # Check for cvip-rag app specifically
        if 'cvip' in app.name.lower():
            print(f"\n   üéØ Found your CVIP RAG app!")
            print(f"   Status: {app.status}")
            
            if hasattr(app.status, 'message'):
                print(f"   Message: {app.status.message}")
    
except Exception as e:
    print(f"‚ùå Could not check app status: {e}")
    print("\nüí° The app might still be deploying...")

print("\n" + "=" * 70)

# COMMAND ----------

import os
os.environ["DATABRICKS_HOST"] = "https://YOUR_WORKSPACE.cloud.databricks.com"

rag = FinalProductionRAG(
    enable_reranking=False,
    enable_persistence=True,
    flush_every_n=3,
    flush_every_seconds=30
)

print("‚úÖ rag is ready!")

# COMMAND ----------

# ============================================================
# CVIP RAG - INTERACTIVE DEMO + MAIN LAUNCHER
# ============================================================
import time

# ============================================================
# ANALYTICS & FAREWELL FUNCTIONS
# ============================================================

def get_session_analytics(session_id="interactive_demo"):
    summary = rag.get_session_summary(session_id)
    session = rag.sessions.get(session_id)
    
    avg_latency = 0
    if session:
        avg_latency = session.health_stats.get('avg_latency', 0) / 1000
    
    total    = summary.get('total_turns', 0)
    domain   = summary.get('domain_queries', 0)
    memory   = summary.get('memory_queries', 0)
    ood      = total - domain - memory
    duration = summary.get('session_duration', '0:00:00')
    
    cited = 0
    if session:
        cited = sum(
            1 for t in session.memory.full_session_log
            if t.get('metadata', {}).get('citations')
        )
    citation_rate = round(cited / max(domain, 1) * 100, 1) if domain > 0 else 0
    
    return {
        'total_questions':         total,
        'domain_questions':        domain,
        'memory_questions':        memory,
        'out_of_domain_questions': ood,
        'avg_processing_time':     round(avg_latency, 2),
        'citation_rate':           citation_rate,
        'session_duration':        duration
    }


def display_farewell_summary(session_id="interactive_demo"):
    analytics = get_session_analytics(session_id)
    
    print("\n" + "üéì" * 25)
    print("     THANK YOU FOR USING THE CVIP RAG SYSTEM!")
    print("üéì" * 25)
    
    print(f"\nüìä SESSION ANALYTICS")
    print("=" * 60)
    print(f"üïí Duration         : {analytics['session_duration']}")
    print(f"‚ùì Total Questions  : {analytics['total_questions']}")
    print(f"üî¨ Domain Questions : {analytics['domain_questions']}")
    print(f"üß† Memory Questions : {analytics['memory_questions']}")
    print(f"‚ö†Ô∏è  Out-of-Domain   : {analytics['out_of_domain_questions']}")
    print(f"‚ö° Avg Response Time: {analytics['avg_processing_time']}s")
    print(f"üìö Citation Rate    : {analytics['citation_rate']}%")
    
    session = rag.sessions.get(session_id)
    if session and session.memory.full_session_log:
        print(f"\nüìù YOUR QUESTIONS:")
        print("=" * 60)
        for t in session.memory.full_session_log:
            qc = t.get('metadata', {}).get('query_classification', {})
            if qc.get('is_memory_query'):
                icon = "üß†"
            elif qc.get('is_domain_relevant'):
                icon = "üî¨"
            else:
                icon = "‚ö†Ô∏è "
            print(f"  {t['turn_number']:2}. {icon} {t['query']}")
    
    print(f"\nüéØ SESSION INSIGHTS:")
    print("=" * 40)
    t = analytics['avg_processing_time']
    if t < 2:   print("  ‚ö° Excellent response times!")
    elif t < 5: print("  ‚è±Ô∏è  Good performance maintained")
    else:       print("  üêå Response times were high ‚Äî likely LLM latency")
    
    cr = analytics['citation_rate']
    if cr > 80:   print("  üìö High-quality citations provided")
    elif cr > 50: print("  üìö Good source attribution")
    else:         print("  üìö Try more specific CVIP questions for better citations")
    
    if analytics['domain_questions'] > analytics['out_of_domain_questions']:
        print("  üéØ Great focus on relevant CVIP topics!")
    
    print(f"\nüåü FINAL MESSAGE:")
    print("=" * 30)
    n = analytics['total_questions']
    if n >= 8:   print("  üèÜ Outstanding engagement with the system!")
    elif n >= 4: print("  üëç Good exploration of the topics!")
    else:        print("  üå± Thanks for trying the system!")
    
    print("  üî¨ Keep exploring Image Processing and Computer Vision!")
    print("  üìö Knowledge base: 10,097 chunks | Textbooks + Papers")
    print(f"\nüëã Goodbye and happy learning!")
    print("üéì" * 25)


def display_realtime_stats(session_id="interactive_demo"):
    analytics = get_session_analytics(session_id)
    print(f"\nüìä REAL-TIME STATS:")
    print(f"  Questions  : {analytics['total_questions']}")
    print(f"  Domain     : {analytics['domain_questions']}")
    print(f"  Memory     : {analytics['memory_questions']}")
    print(f"  Out-Domain : {analytics['out_of_domain_questions']}")
    print(f"  Avg Time   : {analytics['avg_processing_time']}s")
    print(f"  Citations  : {analytics['citation_rate']}%")
    print(f"  Duration   : {analytics['session_duration']}")


# ============================================================
# MODE FUNCTIONS
# ============================================================

def quick_demo(session_id="quick_demo"):
    """Quick demonstration of all system features."""
    print("\nüß™ CVIP RAG - Quick Feature Demo")
    print("=" * 60)
    print("üìö Testing all query types: Domain | General | Memory | OOD")
    print("=" * 60)
    
    demo_questions = [
        ("üî¨ Domain",  "What is digital image processing?"),
        ("üî¨ Domain",  "Explain edge detection with an example"),
        ("üî¨ Domain",  "How does the Sobel operator work?"),
        ("üåç General", "What is the history of digital image processing?"),
        ("üåç General", "What is the capital of Andhra Pradesh?"),
        ("üö´ OOD",     "How does deeplearning effect computer vision?"),
        ("üß† Memory",  "What was my first question?"),
        ("üß† Memory",  "List all the questions I asked"),
    ]
    
    for label, question in demo_questions:
        print(f"\n{label}: {question}")
        print("-" * 50)
        
        start    = time.time()
        response = rag.ask(query=question, session_id=session_id)
        elapsed  = time.time() - start
        
        answer = response['answer']
        if len(answer) > 300:
            answer = answer[:300] + "..."
        
        print(f"ü§ñ {answer}")
        
        if response.get('citations'):
            print(f"üìö Sources: {', '.join(response['citations'][:2])}")
        
        print(f"üìä {response['support_level']} | "
              f"{response['confidence']:.0%} | "
              f"{elapsed:.1f}s")
        
        time.sleep(0.5)
    
    print("\n" + "=" * 60)
    print("üìä DEMO COMPLETE ‚Äî Showing Analytics:")
    display_farewell_summary(session_id)


def interactive_mode(session_id="interactive_demo"):
    """Full interactive mode ‚Äî domain, general, memory all work naturally."""
    print("\nü§ñ CVIP RAG System - Interactive Mode")
    print("=" * 70)
    print("üìö 10,097 chunks | LLaMA 3.3 70B | BGE-Large embeddings")
    print("=" * 70)
    print("üí° Ask anything ‚Äî CVIP topics, general knowledge, or memory questions")
    print("   e.g. 'What was my third question?' works naturally mid-conversation")
    print("   Commands: 'quit' = full summary | 'stats' = live stats")
    print("=" * 70)
    
    while True:
        try:
            query = input("\n‚ùì Your question: ").strip()
        except EOFError:
            break
        
        if not query:
            continue
        
        if query.lower() in ['quit', 'exit', 'q']:
            print("\nüîÑ Generating session summary...")
            display_farewell_summary(session_id)
            break
        
        if query.lower() == 'stats':
            display_realtime_stats(session_id)
            continue
        
        response = rag.ask(query=query, session_id=session_id)
        
        print(f"\nü§ñ ANSWER:")
        print(response['answer'])
        
        if response.get('citations'):
            print(f"\nüìö SOURCES:")
            for i, c in enumerate(response['citations'], 1):
                print(f"   {i}. {c}")
        
        print(f"\nüìä Confidence: {response['confidence']:.1%} | "
              f"Support: {response['support_level']} | "
              f"Latency: {response['latency_ms']}ms")
        print("-" * 70)


# ============================================================
# MAIN LAUNCHER
# ============================================================

print("\n" + "üéì" * 35)
print("\n  ü§ñ CVIP RAG SYSTEM ‚Äî Powered by Databricks + LLaMA 3.3 70B")
print("\n" + "üéì" * 35)

print("""
üéØ Versatile RAG System
Enhanced capabilities:
  ‚úÖ Technical CVIP questions with citations
  ‚úÖ General knowledge questions
  ‚úÖ Memory recall ‚Äî ask about any previous question naturally
  ‚úÖ Out-of-domain detection

Choose an option:
  1. Interactive Mode    (Full Experience)
  2. Quick Feature Demo (Automated showcase)
""")

choice = input("Enter choice (1 or 2): ").strip()

if choice == "1":
    interactive_mode()
elif choice == "2":
    quick_demo()
else:
    print("‚ùå Invalid choice ‚Äî launching Interactive Mode by default")
    interactive_mode()

# COMMAND ----------

import requests, base64, getpass, re

GITHUB_TOKEN = getpass.getpass("GitHub token: ")
REPO   = "Dhanushhuu/Dhanush-demo"
BRANCH = "main"

# Step 1: Re-export notebook
token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
host  = "https://YOUR_WORKSPACE.cloud.databricks.com"
path  = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()

r = requests.get(
    f"{host}/api/2.0/workspace/export",
    headers={"Authorization": f"Bearer {token}"},
    params={"path": path, "format": "SOURCE"}
)
source = base64.b64decode(r.json()["content"]).decode("utf-8")
print(f"‚úÖ Exported: {len(source)} chars")

# Step 2: Scrub ALL secrets
patterns = [
    (r'github_pat_[A-Za-z0-9_]{80,}',                   'GITHUB_TOKEN_REMOVED'),
    (r'ghp_[A-Za-z0-9]{36,}',                           'GITHUB_TOKEN_REMOVED'),
    (r'dapi[a-zA-Z0-9]{32,}',                           'DATABRICKS_TOKEN_REMOVED'),
    (r'https://dbc-[a-zA-Z0-9\-]+\.cloud\.databricks\.com',
                                                          'https://YOUR_WORKSPACE.cloud.databricks.com'),
]
for pattern, replacement in patterns:
    matches = re.findall(pattern, source)
    if matches:
        print(f"üîç Scrubbed {len(matches)}x: {pattern[:40]}")
    source = re.sub(pattern, replacement, source)

print(f"‚úÖ Scrubbed: {len(source)} chars")

# Step 3: Push
def push(filename, content_str):
    content_bytes = base64.b64encode(content_str.encode()).decode()
    r = requests.get(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"}
    )
    sha = r.json().get("sha") if r.status_code == 200 else None
    payload = {
        "message": f"Update {filename} ‚Äî latest RAG system",
        "content": content_bytes,
        "branch":  BRANCH
    }
    if sha:
        payload["sha"] = sha
    r = requests.put(
        f"https://api.github.com/repos/{REPO}/contents/{filename}",
        headers={"Authorization": f"token {GITHUB_TOKEN}"},
        json=payload,
        timeout=120
    )
    print(f"{'‚úÖ' if r.status_code in [200,201] else '‚ùå'} {filename}: {r.status_code}")
    if r.status_code not in [200, 201]:
        print(r.text[:300])

push("rag_components.py", source)

print("\nüéâ Done! Revoke your token now:")
print("github.com ‚Üí Settings ‚Üí Developer settings ‚Üí Fine-grained tokens ‚Üí Revoke")